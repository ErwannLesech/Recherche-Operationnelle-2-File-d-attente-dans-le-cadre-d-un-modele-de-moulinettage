"""
Application Streamlit pour la simulation de la moulinette EPITA.

Interface interactive permettant de:
- Visualiser le mod√®le Waterfall (files infinies/finies)
- Simuler les m√©canismes de backup et leur impact
- Analyser les populations diff√©renci√©es (Channels & Dams)
- Optimiser co√ªt/qualit√© de service
- Obtenir des recommandations de scaling

Bas√© sur le rapport de projet ERO2 - Janvier 2026

Lancer avec: streamlit run app/frontend/main_app.py
"""

import streamlit as st
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import sys
from pathlib import Path
from typing import Callable, Dict, List, Tuple, Optional
import math

# Ajouter le chemin parent pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from app.models.base_queue import GenericQueue, ChainQueue, QueueMetrics, SimulationResults
from app.personas import PersonaFactory, StudentType
from app.personas.usage_patterns import AcademicPeriod
from app.simulation import RushSimulator, MoulinetteSystem, SimulationConfig, ServerConfig
from app.optimization import CostOptimizer, ScalingAdvisor, CostModel, ScalingPolicy
import json
from datetime import datetime
import os


def apply_dark_theme(fig):
    """Applique un th√®me sombre aux graphiques Plotly pour am√©liorer la lisibilit√©."""
    fig.update_layout(
        plot_bgcolor='#1e1e1e',
        paper_bgcolor='#2d2d2d',
        font=dict(color='#e0e0e0'),
        xaxis=dict(
            gridcolor='#404040',
            zerolinecolor='#404040'
        ),
        yaxis=dict(
            gridcolor='#404040',
            zerolinecolor='#404040'
        ),
        legend=dict(
            bgcolor='rgba(45, 45, 45, 0.8)',
            bordercolor='#404040',
            borderwidth=1
        )
    )
    return fig


def run_app():
    """Point d'entr√©e principal."""
    main()


def main():
    """Application principale."""
    st.set_page_config(
        page_title="Moulinette Simulator - EPITA",
        page_icon="üéØ",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # CSS personnalis√©
    st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 0.5rem;
    }
    .sub-header {
        font-size: 1rem;
        color: #666;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
    }
    .scenario-box {
        background: #2b3e50;
        border-left: 4px solid #1f77b4;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 8px 8px 0;
        color: #ecf0f1;
    }
    .formula-box {
        background: #34495e;
        border: 1px solid #3498db;
        padding: 1rem;
        border-radius: 8px;
        font-family: 'Courier New', monospace;
        text-align: center;
        margin: 1rem 0;
        color: #ecf0f1;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 1rem;
    }
    </style>
    """, unsafe_allow_html=True)
    
    st.markdown('<p class="main-header">üéØ Moulinette Simulator</p>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">Optimisation des files d\'attente pour la moulinette EPITA - Projet ERO2</p>', unsafe_allow_html=True)
    
    # Sidebar pour les param√®tres globaux
    with st.sidebar:
        st.header("‚öôÔ∏è Param√®tres Globaux")
        st.markdown("---")
        
        st.subheader("File 1: Ex√©cution Tests")
        mu_rate1 = st.slider(
            "Œº‚ÇÅ - Taux de service (tests/min)",
            min_value=1.0, max_value=50.0, value=10.0, step=0.5,
            help="Vitesse de traitement des test-suites par serveur"
        )
        n_servers = st.slider(
            "c - Nombre de runners",
            min_value=1, max_value=20, value=4
        )
        K1 = st.slider(
            "K‚ÇÅ - Capacit√© buffer File 1",
            min_value=10, max_value=1000, value=100, step=10
        )
        
        st.markdown("---")
        st.subheader("File 2: Renvoi R√©sultats")
        mu_rate2 = st.slider(
            "Œº‚ÇÇ - Taux de service (r√©sultats/min)",
            min_value=1.0, max_value=100.0, value=20.0, step=1.0,
            help="Vitesse d'envoi des r√©sultats au frontend"
        )
        K2 = st.slider(
            "K‚ÇÇ - Capacit√© buffer File 2",
            min_value=10, max_value=500, value=50, step=10
        )
        
        st.markdown("---")
        st.subheader("Mod√®le File 2")
        file2_model = st.radio(
            "Type de service",
            ["M/M/1 (Exponentiel)", "M/D/1 (D√©terministe)"],
            help="M/D/1 r√©duit le temps d'attente de 50% vs M/M/1"
        )
    
    # Onglets principaux
    tabs = st.tabs([
        "üìä Sc√©nario 1: Waterfall",
        "üíæ Sc√©nario 2: Backup",
        "üë• Channels & Dams",
        "üí∞ Optimisation Co√ªt/QoS",
        "üìà Auto-Scaling",
        "üî¨ Benchmark Mod√®les"
    ])
    
    with tabs[0]:
        render_waterfall_scenario(mu_rate1, mu_rate2, n_servers, K1, K2, file2_model)
    
    with tabs[1]:
        render_backup_scenario(mu_rate1, mu_rate2, n_servers, K1, K2)
    
    with tabs[2]:
        render_channels_dams_tab(mu_rate1, n_servers, K1)
    
    with tabs[3]:
        render_optimization_tab(mu_rate1, n_servers, K1)
    
    with tabs[4]:
        render_autoscaling_tab(mu_rate1, mu_rate2, n_servers, K1, K2)
    
    with tabs[5]:
        render_benchmark_tab(mu_rate1, n_servers, K1)


# ==============================================================================
# SC√âNARIO 1: MOD√àLE WATERFALL
# ==============================================================================

def render_waterfall_scenario(mu_rate1: float, mu_rate2: float, n_servers: int, K1: int, K2: int, file2_model: str):
    """Sc√©nario 1: Mod√®le Waterfall avec files infinies puis finies."""
    st.header("üìä Sc√©nario 1: Mod√®le Waterfall")
    
    st.markdown("""
    <div class="scenario-box">
    <strong>Architecture en cascade de la moulinette:</strong><br>
    <code>√âtudiants ‚Üí Buffer‚ÇÅ (K‚ÇÅ) ‚Üí Runners (c) ‚Üí Buffer‚ÇÇ (K‚ÇÇ) ‚Üí Frontend (1 serveur)</code>
    </div>
    """, unsafe_allow_html=True)
    
    # Sch√©ma du syst√®me
    col_diagram, col_params = st.columns([2, 1])
    
    with col_params:
        st.subheader("Param√®tres d'entr√©e")
        lambda_rate = st.slider(
            "Œª - Taux d'arriv√©e (tags/min)",
            min_value=1.0, max_value=100.0, value=30.0, step=1.0,
            key="waterfall_lambda"
        )
        
        capacity_mode = st.radio(
            "Mode de capacit√©",
            ["Files infinies (K=‚àû)", "Files finies (K limit√©)"],
            key="waterfall_capacity"
        )
        
        use_md1 = "M/D/1" in file2_model
    
    with col_diagram:
        # Cr√©er un sch√©ma visuel du syst√®me
        fig_diagram = create_waterfall_diagram(lambda_rate, mu_rate1, mu_rate2, n_servers, K1, K2, capacity_mode)
        st.plotly_chart(apply_dark_theme(fig_diagram), use_container_width=True)
    
    st.divider()
    
    # Section analyse th√©orique
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìê File 1: Ex√©cution des tests (M/M/c)")
        
        # Condition de stabilit√©
        rho1 = lambda_rate / (n_servers * mu_rate1)
        stable1 = rho1 < 1 or capacity_mode == "Files finies (K limit√©)"
        
        st.markdown(f"""
        **Condition de stabilit√©:** œÅ‚ÇÅ = Œª/(c√óŒº‚ÇÅ) = {lambda_rate:.1f}/({n_servers}√ó{mu_rate1:.1f}) = **{rho1:.3f}**
        """)
        
        if rho1 < 1:
            st.success(f"‚úÖ Syst√®me stable (œÅ‚ÇÅ = {rho1:.2%} < 100%)")
        else:
            if capacity_mode == "Files finies (K limit√©)":
                st.warning(f"‚ö†Ô∏è œÅ‚ÇÅ = {rho1:.2%} ‚â• 100% mais stable gr√¢ce au buffer fini K‚ÇÅ={K1}")
            else:
                st.error(f"‚ùå Syst√®me instable (œÅ‚ÇÅ = {rho1:.2%} ‚â• 100%)")
        
        if stable1 or capacity_mode == "Files finies (K limit√©)":
            try:
                if capacity_mode == "Files infinies (K=‚àû)":
                    queue1 = GenericQueue(lambda_rate, mu_rate1, f"M/M/{n_servers}", c=n_servers)
                    metrics1 = queue1.compute_theoretical_metrics()
                    P_K1 = 0.0
                    lambda_eff = lambda_rate
                else:
                    # M/M/c/K - calcul avec file finie
                    metrics1, P_K1 = compute_mmck_metrics(lambda_rate, mu_rate1, n_servers, K1)
                    lambda_eff = lambda_rate * (1 - P_K1)
                
                # Afficher les m√©triques
                display_queue_metrics(metrics1, "File 1", P_K1)
                
            except Exception as e:
                st.error(f"Erreur de calcul: {e}")
                metrics1 = None
                lambda_eff = 0
        else:
            metrics1 = None
            lambda_eff = 0
    
    with col2:
        st.subheader(f"üìê File 2: Renvoi des r√©sultats ({'M/D/1' if use_md1 else 'M/M/1'})")
        
        if lambda_eff > 0:
            # Le taux d'entr√©e en file 2 = taux de sortie de file 1
            lambda2 = lambda_eff
            rho2 = lambda2 / mu_rate2
            
            st.markdown(f"""
            **Taux d'arriv√©e effectif:** Œª‚ÇÇ = Œª_eff = {lambda2:.2f} tags/min
            
            **Condition de stabilit√©:** œÅ‚ÇÇ = Œª‚ÇÇ/Œº‚ÇÇ = {lambda2:.2f}/{mu_rate2:.1f} = **{rho2:.3f}**
            """)
            
            if rho2 < 1:
                st.success(f"‚úÖ Syst√®me stable (œÅ‚ÇÇ = {rho2:.2%} < 100%)")
            else:
                if capacity_mode == "Files finies (K limit√©)":
                    st.warning(f"‚ö†Ô∏è œÅ‚ÇÇ = {rho2:.2%} ‚â• 100% mais stable gr√¢ce au buffer fini K‚ÇÇ={K2}")
                else:
                    st.error(f"‚ùå Syst√®me instable (œÅ‚ÇÇ = {rho2:.2%} ‚â• 100%)")
            
            try:
                if capacity_mode == "Files infinies (K=‚àû)" and rho2 < 1:
                    notation = "M/D/1" if use_md1 else "M/M/1"
                    queue2 = GenericQueue(lambda2, mu_rate2, notation)
                    metrics2 = queue2.compute_theoretical_metrics()
                    P_K2 = 0.0
                else:
                    # M/M/1/K ou M/D/1/K
                    metrics2, P_K2 = compute_mm1k_metrics(lambda2, mu_rate2, K2, use_md1)
                
                display_queue_metrics(metrics2, "File 2", P_K2)
                
                # Avantage M/D/1 vs M/M/1
                if use_md1:
                    st.info("üí° **M/D/1:** Temps d'attente r√©duit de ~50% gr√¢ce au service d√©terministe")
                
            except Exception as e:
                st.error(f"Erreur de calcul: {e}")
                metrics2 = None
                P_K2 = 0
        else:
            st.warning("Calculez d'abord la File 1")
            metrics2 = None
            P_K2 = 0
    
    st.divider()
    
    # M√©triques globales du syst√®me
    st.subheader("üéØ Performance Globale du Syst√®me Waterfall")
    
    if metrics1 is not None and 'metrics2' in dir() and metrics2 is not None:
        col_m1, col_m2, col_m3, col_m4 = st.columns(4)
        
        W_total = metrics1.W + metrics2.W
        P_rejet = P_K1 if 'P_K1' in dir() else 0.0
        P_blank = (1 - P_rejet) * P_K2 if 'P_K2' in dir() else 0.0
        
        with col_m1:
            st.metric("Temps de s√©jour total (W)", f"{W_total:.3f} min")
        with col_m2:
            st.metric("Taux de rejet (P_K‚ÇÅ)", f"{P_rejet:.2%}")
        with col_m3:
            st.metric("Pages blanches (P_blank)", f"{P_blank:.4%}")
        with col_m4:
            throughput = lambda_rate * (1 - P_rejet) * (1 - P_K2)
            st.metric("D√©bit effectif", f"{throughput:.2f} tags/min")
        
        # Formules
        st.markdown("""
        <div class="formula-box">
        <strong>Formules cl√©s:</strong><br>
        W_total = W‚ÇÅ + W‚ÇÇ<br>
        P_rejet = P(file 1 pleine) = œÄ_K‚ÇÅ<br>
        P_blank = (1 - P_rejet) √ó P_K‚ÇÇ
        </div>
        """, unsafe_allow_html=True)
    
    # Simulation Monte Carlo
    st.divider()
    st.subheader("üé≤ Simulation Monte Carlo")
    
    col_sim1, col_sim2 = st.columns([1, 2])
    
    with col_sim1:
        n_customers = st.number_input("Nombre de clients", 100, 10000, 2000, step=100, key="waterfall_n")
        n_runs = st.number_input("Nombre de r√©p√©titions", 1, 20, 5, key="waterfall_runs")
        run_sim = st.button("‚ñ∂Ô∏è Lancer simulation Waterfall", type="primary", key="waterfall_run")
    
    with col_sim2:
        if run_sim:
            run_waterfall_simulation(lambda_rate, mu_rate1, mu_rate2, n_servers, K1, K2, 
                                    n_customers, n_runs, capacity_mode, use_md1)


def create_waterfall_diagram(lambda_rate, mu_rate1, mu_rate2, n_servers, K1, K2, capacity_mode):
    """Cr√©e un sch√©ma visuel du syst√®me Waterfall."""
    fig = go.Figure()
    
    # Param√®tres de positionnement
    y_base = 0.5
    
    # Source (√©tudiants)
    fig.add_trace(go.Scatter(
        x=[0], y=[y_base],
        mode='markers+text',
        marker=dict(size=40, color='#2ecc71', symbol='circle'),
        text=['üë®‚Äçüéì'], textposition='middle center',
        name='√âtudiants', showlegend=False
    ))
    fig.add_annotation(x=0, y=y_base-0.15, text=f"Œª={lambda_rate}", showarrow=False)
    
    # Fl√®che vers Buffer 1
    fig.add_annotation(x=0.5, y=y_base, ax=0.15, ay=y_base,
                      xref="x", yref="y", axref="x", ayref="y",
                      showarrow=True, arrowhead=2, arrowsize=1.5)
    
    # Buffer 1
    k1_text = "‚àû" if "infinies" in capacity_mode else str(K1)
    fig.add_trace(go.Scatter(
        x=[1], y=[y_base],
        mode='markers+text',
        marker=dict(size=50, color='#3498db', symbol='square'),
        text=[f'üì¶'], textposition='middle center',
        name='Buffer 1', showlegend=False
    ))
    fig.add_annotation(x=1, y=y_base-0.15, text=f"K‚ÇÅ={k1_text}", showarrow=False)
    
    # Fl√®che vers Runners
    fig.add_annotation(x=1.5, y=y_base, ax=1.15, ay=y_base,
                      xref="x", yref="y", axref="x", ayref="y",
                      showarrow=True, arrowhead=2, arrowsize=1.5)
    
    # Runners (multi-serveurs)
    fig.add_trace(go.Scatter(
        x=[2], y=[y_base],
        mode='markers+text',
        marker=dict(size=60, color='#e74c3c', symbol='square'),
        text=['üñ•Ô∏è'], textposition='middle center',
        name='Runners', showlegend=False
    ))
    fig.add_annotation(x=2, y=y_base-0.15, text=f"c={n_servers}, Œº‚ÇÅ={mu_rate1}", showarrow=False)
    
    # Fl√®che vers Buffer 2
    fig.add_annotation(x=2.5, y=y_base, ax=2.15, ay=y_base,
                      xref="x", yref="y", axref="x", ayref="y",
                      showarrow=True, arrowhead=2, arrowsize=1.5)
    
    # Buffer 2
    k2_text = "‚àû" if "infinies" in capacity_mode else str(K2)
    fig.add_trace(go.Scatter(
        x=[3], y=[y_base],
        mode='markers+text',
        marker=dict(size=50, color='#9b59b6', symbol='square'),
        text=['üì¶'], textposition='middle center',
        name='Buffer 2', showlegend=False
    ))
    fig.add_annotation(x=3, y=y_base-0.15, text=f"K‚ÇÇ={k2_text}", showarrow=False)
    
    # Fl√®che vers Frontend
    fig.add_annotation(x=3.5, y=y_base, ax=3.15, ay=y_base,
                      xref="x", yref="y", axref="x", ayref="y",
                      showarrow=True, arrowhead=2, arrowsize=1.5)
    
    # Frontend
    fig.add_trace(go.Scatter(
        x=[4], y=[y_base],
        mode='markers+text',
        marker=dict(size=50, color='#f39c12', symbol='circle'),
        text=['üñ•Ô∏è'], textposition='middle center',
        name='Frontend', showlegend=False
    ))
    fig.add_annotation(x=4, y=y_base-0.15, text=f"Œº‚ÇÇ={mu_rate2}", showarrow=False)
    
    fig.update_layout(
        height=200,
        xaxis=dict(showgrid=False, showticklabels=False, range=[-0.5, 4.5]),
        yaxis=dict(showgrid=False, showticklabels=False, range=[0, 1]),
        margin=dict(l=20, r=20, t=30, b=20),
        title="Architecture du syst√®me Waterfall"
    )
    
    return apply_dark_theme(fig)


def display_queue_metrics(metrics: QueueMetrics, name: str, P_K: float = 0.0):
    """Affiche les m√©triques d'une file."""
    col1, col2 = st.columns(2)
    with col1:
        st.metric(f"L (clients dans syst√®me)", f"{metrics.L:.2f}")
        st.metric(f"W (temps de s√©jour)", f"{metrics.W:.3f} min")
    with col2:
        st.metric(f"Lq (en attente)", f"{metrics.Lq:.2f}")
        st.metric(f"Wq (temps d'attente)", f"{metrics.Wq:.3f} min")
    
    if P_K > 0:
        st.metric(f"P(blocage)", f"{P_K:.4%}")


def compute_mmck_metrics(lambda_rate: float, mu_rate: float, c: int, K: int) -> Tuple[QueueMetrics, float]:
    """Calcule les m√©triques pour M/M/c/K."""
    rho = lambda_rate / (c * mu_rate)
    a = lambda_rate / mu_rate
    
    # Calcul de œÄ‚ÇÄ
    sum1 = sum((a ** n) / math.factorial(n) for n in range(c))
    if rho != 1:
        sum2 = ((a ** c) / math.factorial(c)) * (1 - rho ** (K - c + 1)) / (1 - rho)
    else:
        sum2 = ((a ** c) / math.factorial(c)) * (K - c + 1)
    
    P0 = 1 / (sum1 + sum2)
    
    # Calcul de œÄ_K (probabilit√© de blocage)
    if rho != 1:
        P_K = ((a ** K) / (math.factorial(c) * (c ** (K - c)))) * P0
    else:
        P_K = ((a ** K) / math.factorial(K)) * P0
    
    # Lambda effectif
    lambda_eff = lambda_rate * (1 - P_K)
    
    # Calcul de L
    L = 0
    for n in range(1, c + 1):
        L += n * ((a ** n) / math.factorial(n)) * P0
    
    if rho != 1:
        numerator = (a ** c * rho * P0) / math.factorial(c)
        L += numerator * (1 - rho ** (K - c + 1) - (1 - rho) * (K - c + 1) * rho ** (K - c)) / ((1 - rho) ** 2)
    
    # M√©triques via Little
    Lq = max(0, L - (1 - P0) * c / c)  # Approximation
    W = L / lambda_eff if lambda_eff > 0 else 0
    Wq = Lq / lambda_eff if lambda_eff > 0 else 0
    
    metrics = QueueMetrics(
        rho=rho if rho < 1 else 1.0,
        L=L,
        Lq=Lq,
        W=W,
        Wq=Wq,
        Ws=1/mu_rate,
        P0=P0,
        Pk=P_K,
        lambda_eff=lambda_eff,
        throughput=lambda_eff
    )
    
    return metrics, P_K


def compute_mm1k_metrics(lambda_rate: float, mu_rate: float, K: int, deterministic: bool = False) -> Tuple[QueueMetrics, float]:
    """Calcule les m√©triques pour M/M/1/K ou M/D/1/K."""
    rho = lambda_rate / mu_rate
    
    if rho == 1:
        P0 = 1 / (K + 1)
        P_K = 1 / (K + 1)
        L = K / 2
    else:
        P0 = (1 - rho) / (1 - rho ** (K + 1))
        P_K = P0 * (rho ** K)
        L = rho * (1 - (K + 1) * rho ** K + K * rho ** (K + 1)) / ((1 - rho) * (1 - rho ** (K + 1)))
    
    lambda_eff = lambda_rate * (1 - P_K)
    
    W = L / lambda_eff if lambda_eff > 0 else 0
    Wq = W - 1/mu_rate
    Lq = lambda_eff * Wq if Wq > 0 else 0
    
    # Pour M/D/1: r√©duction de 50% du temps d'attente
    if deterministic:
        Wq = Wq / 2
        Lq = Lq / 2
        W = Wq + 1/mu_rate
    
    metrics = QueueMetrics(
        rho=min(rho, 1.0),
        L=L,
        Lq=Lq,
        W=W,
        Wq=Wq,
        Ws=1/mu_rate,
        P0=P0,
        Pk=P_K,
        lambda_eff=lambda_eff,
        throughput=lambda_eff
    )
    
    return metrics, P_K


def run_waterfall_simulation(lambda_rate, mu_rate1, mu_rate2, n_servers, K1, K2, 
                             n_customers, n_runs, capacity_mode, use_md1):
    """Ex√©cute une simulation Monte Carlo du syst√®me Waterfall."""
    with st.spinner("Simulation en cours..."):
        results_all = []
        
        for run in range(n_runs):
            # File 1: M/M/c ou M/M/c/K
            if "infinies" in capacity_mode:
                queue1 = GenericQueue(lambda_rate, mu_rate1, f"M/M/{n_servers}", c=n_servers)
            else:
                queue1 = GenericQueue(lambda_rate, mu_rate1, f"M/M/{n_servers}", c=n_servers, K=K1)
            
            res1 = queue1.simulate(n_customers=n_customers)
            
            # File 2: utilise les temps de d√©part de File 1 comme arriv√©es
            if len(res1.departure_times) > 0:
                notation2 = "M/D/1" if use_md1 else "M/M/1"
                if "infinies" in capacity_mode:
                    queue2 = GenericQueue(lambda_rate, mu_rate2, notation2)
                else:
                    queue2 = GenericQueue(lambda_rate, mu_rate2, notation2, K=K2)
                
                res2 = queue2.simulate(external_arrival_times=res1.departure_times)
                
                results_all.append({
                    'run': run + 1,
                    'file1_served': res1.n_served,
                    'file1_rejected': res1.n_rejected,
                    'file1_wait': np.mean(res1.waiting_times) if len(res1.waiting_times) > 0 else 0,
                    'file1_system': np.mean(res1.system_times) if len(res1.system_times) > 0 else 0,
                    'file2_served': res2.n_served,
                    'file2_rejected': res2.n_rejected,
                    'file2_wait': np.mean(res2.waiting_times) if len(res2.waiting_times) > 0 else 0,
                    'file2_system': np.mean(res2.system_times) if len(res2.system_times) > 0 else 0,
                    'total_wait': (np.mean(res1.waiting_times) if len(res1.waiting_times) > 0 else 0) + 
                                  (np.mean(res2.waiting_times) if len(res2.waiting_times) > 0 else 0),
                    'total_system': (np.mean(res1.system_times) if len(res1.system_times) > 0 else 0) + 
                                    (np.mean(res2.system_times) if len(res2.system_times) > 0 else 0),
                })
        
        if results_all:
            df = pd.DataFrame(results_all)
            
            # Statistiques
            st.markdown("### R√©sultats de simulation")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Temps d'attente moyen total", 
                         f"{df['total_wait'].mean():.3f} ¬± {df['total_wait'].std():.3f} min")
            with col2:
                st.metric("Temps de s√©jour moyen total",
                         f"{df['total_system'].mean():.3f} ¬± {df['total_system'].std():.3f} min")
            with col3:
                reject_rate = df['file1_rejected'].sum() / (df['file1_served'].sum() + df['file1_rejected'].sum())
                st.metric("Taux de rejet File 1", f"{reject_rate:.2%}")
            
            # Graphique
            fig = make_subplots(rows=1, cols=2, subplot_titles=['Temps par file', 'Distribution des temps totaux'])
            
            fig.add_trace(go.Box(y=df['file1_wait'], name='Attente F1'), row=1, col=1)
            fig.add_trace(go.Box(y=df['file2_wait'], name='Attente F2'), row=1, col=1)
            fig.add_trace(go.Box(y=df['file1_system'], name='S√©jour F1'), row=1, col=1)
            fig.add_trace(go.Box(y=df['file2_system'], name='S√©jour F2'), row=1, col=1)
            
            fig.add_trace(go.Histogram(x=df['total_system'], name='Temps total', nbinsx=20), row=1, col=2)
            
            fig.update_layout(height=400, showlegend=True)
            st.plotly_chart(apply_dark_theme(fig), use_container_width=True)


# ==============================================================================
# SC√âNARIO 2: M√âCANISMES DE BACKUP
# ==============================================================================

def render_backup_scenario(mu_rate1: float, mu_rate2: float, n_servers: int, K1: int, K2: int):
    """Sc√©nario 2: Impact du backup sur les pages blanches."""
    st.header("üíæ Sc√©nario 2: M√©canismes de Backup")
    
    st.markdown("""
    <div class="scenario-box">
    <strong>Probl√©matique:</strong> Quand la file 2 est satur√©e, les r√©sultats sont perdus ‚Üí <em>pages blanches</em><br><br>
    <strong>Solution:</strong> Sauvegarder les r√©sultats avant insertion dans la file 2
    </div>
    """, unsafe_allow_html=True)
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("üìä Sans backup")
        
        lambda_rate = st.slider("Œª - Taux d'arriv√©e", 10.0, 100.0, 40.0, key="backup_lambda")
        
        # Calculer les probabilit√©s
        rho2 = lambda_rate / mu_rate2
        
        if rho2 < 1:
            # M/M/1/K2
            P0 = (1 - rho2) / (1 - rho2 ** (K2 + 1))
            P_K2 = P0 * (rho2 ** K2)
        else:
            P_K2 = 1 / (K2 + 1)
        
        # Probabilit√© de page blanche sans backup
        # P_K1 approxim√©e (simplification)
        rho1 = lambda_rate / (n_servers * mu_rate1)
        P_K1 = max(0, min(0.1, (rho1 - 0.8) / 0.2)) if rho1 > 0.8 else 0
        
        P_blank_no_backup = (1 - P_K1) * P_K2
        
        st.metric("Taux de saturation File 2 (œÅ‚ÇÇ)", f"{rho2:.2%}")
        st.metric("P(file 2 pleine)", f"{P_K2:.4%}")
        st.metric("üö® Probabilit√© page blanche", f"{P_blank_no_backup:.4%}")
        
        st.markdown(f"""
        <div class="formula-box">
        P_blank = (1 - P_K‚ÇÅ) √ó P_K‚ÇÇ<br>
        = (1 - {P_K1:.4f}) √ó {P_K2:.4f}<br>
        = <strong>{P_blank_no_backup:.6f}</strong>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.subheader("‚úÖ Avec backup")
        
        backup_mode = st.radio(
            "Type de backup",
            ["Syst√©matique (p=100%)", "Al√©atoire (p variable)"],
            key="backup_mode"
        )
        
        if backup_mode == "Al√©atoire (p variable)":
            p_backup = st.slider("Probabilit√© de backup (p)", 0.0, 1.0, 0.5, 0.05, key="backup_p")
        else:
            p_backup = 1.0
        
        # Impact du backup sur le temps
        mu_backup = st.slider("Œº_backup (sauvegardes/min)", 10.0, 100.0, 50.0, key="backup_mu")
        
        # Nouveau calcul avec backup
        P_blank_with_backup = (1 - P_K1) * P_K2 * (1 - p_backup)
        
        st.metric("Probabilit√© de backup", f"{p_backup:.0%}")
        st.metric("‚úÖ Nouvelle prob. page blanche", f"{P_blank_with_backup:.6%}")
        
        reduction = (P_blank_no_backup - P_blank_with_backup) / P_blank_no_backup * 100 if P_blank_no_backup > 0 else 0
        st.metric("üìâ R√©duction", f"{reduction:.1f}%")
        
        # Temps de backup additionnel
        T_backup = 1 / mu_backup
        st.metric("‚è±Ô∏è Temps de backup ajout√©", f"{T_backup:.3f} min")
    
    st.divider()
    
    # Visualisation
    st.subheader("üìà Impact du backup selon la charge")
    
    lambda_range = np.linspace(10, 100, 50)
    p_blank_no = []
    p_blank_50 = []
    p_blank_100 = []
    
    for lam in lambda_range:
        rho2 = lam / mu_rate2
        if rho2 < 1:
            P0 = (1 - rho2) / (1 - rho2 ** (K2 + 1))
            pk2 = P0 * (rho2 ** K2)
        else:
            pk2 = 1 / (K2 + 1)
        
        rho1 = lam / (n_servers * mu_rate1)
        pk1 = max(0, min(0.1, (rho1 - 0.8) / 0.2)) if rho1 > 0.8 else 0
        
        p_blank_no.append((1 - pk1) * pk2 * 100)
        p_blank_50.append((1 - pk1) * pk2 * 0.5 * 100)
        p_blank_100.append(0)
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=lambda_range, y=p_blank_no, name='Sans backup', line=dict(color='red')))
    fig.add_trace(go.Scatter(x=lambda_range, y=p_blank_50, name='Backup 50%', line=dict(color='orange')))
    fig.add_trace(go.Scatter(x=lambda_range, y=p_blank_100, name='Backup 100%', line=dict(color='green')))
    
    fig.update_layout(
        xaxis_title="Taux d'arriv√©e Œª (tags/min)",
        yaxis_title="Probabilit√© page blanche (%)",
        height=400,
        legend=dict(orientation="h", yanchor="bottom", y=1.02)
    )
    st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
    
    # Trade-off
    st.subheader("‚öñÔ∏è Compromis Backup")
    
    st.markdown("""
    | Aspect | Sans backup | Backup syst√©matique | Backup al√©atoire |
    |--------|-------------|---------------------|------------------|
    | Pages blanches | Possible | **0%** | R√©duit |
    | Latence | Minimale | +T_backup | +p√óT_backup |
    | Stockage | Aucun | Maximum | R√©duit |
    | Complexit√© | Simple | Moyenne | Moyenne |
    """)


# ==============================================================================
# CHANNELS & DAMS: POPULATIONS DIFF√âRENCI√âES
# ==============================================================================

def render_channels_dams_tab(mu_rate1: float, n_servers: int, K1: int):
    """Onglet Channels & Dams pour populations diff√©renci√©es."""
    st.header("üë• Channels & Dams: Populations Diff√©renci√©es")
    
    st.markdown("""
    <div class="scenario-box">
    <strong>Constat:</strong> Les diff√©rentes populations ont des besoins distincts<br>
    <ul>
        <li><strong>Pr√©pa SUP/SP√â</strong>: Soumissions group√©es √† la deadline</li>
        <li><strong>ING1</strong>: Soumissions fr√©quentes, traitement imm√©diat</li>
        <li><strong>Admin/Staff</strong>: Priorit√© pour tests continus</li>
    </ul>
    </div>
    """, unsafe_allow_html=True)
    
    # Cr√©er les personas
    personas = PersonaFactory.create_all_personas()
    
    # Afficher les caract√©ristiques
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("üìä Caract√©ristiques des populations")
        
        data = []
        for student_type, persona in personas.items():
            data.append({
                'Population': persona.name,
                'Effectif': persona.population_size,
                'Taux base (sub/h)': persona.base_submission_rate,
                'Variance': f"{persona.variance_coefficient:.2f}",
                'Procrastination': f"{persona.procrastination_level:.0%}"
            })
        
        df = pd.DataFrame(data)
        st.dataframe(df, use_container_width=True)
    
    with col2:
        st.subheader("üìà Patterns d'arriv√©e sur 24h")
        
        hours = list(range(24))
        fig = go.Figure()
        
        for student_type, persona in personas.items():
            rates = [persona.get_arrival_rate(h) for h in hours]
            fig.add_trace(go.Scatter(
                x=hours, y=rates,
                mode='lines+markers',
                name=persona.name,
                line=dict(width=2)
            ))
        
        fig.update_layout(
            xaxis_title='Heure',
            yaxis_title='Taux d\'arriv√©e (sub/h)',
            height=350,
            legend=dict(orientation="h", yanchor="bottom", y=1.02)
        )
        st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
    
    st.divider()
    
    # Strat√©gies de r√©gulation
    st.subheader("üéõÔ∏è Strat√©gies de R√©gulation")
    
    strategy = st.radio(
        "Strat√©gie",
        ["File unique (actuel)", "Dam - Blocage p√©riodique", "Channels - Files s√©par√©es", "Hybride - Pool avec priorit√©s"],
        horizontal=True
    )
    
    if strategy == "Dam - Blocage p√©riodique":
        st.markdown("""
        **Principe:** Bloquer p√©riodiquement la moulinette pour grouper les traitements
        
        - Ferm√©e pendant t_b
        - Ouverte pendant t_b/2
        """)
        
        t_block = st.slider("Dur√©e de blocage t_b (min)", 5, 60, 15)
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("P√©riode ferm√©e", f"{t_block} min")
        with col2:
            st.metric("P√©riode ouverte", f"{t_block/2} min")
        
        # Visualisation du cycle
        fig = go.Figure()
        x = []
        y = []
        for i in range(5):
            # Ferm√©
            x.extend([i * 1.5 * t_block, i * 1.5 * t_block + t_block])
            y.extend([0, 0])
            # Ouvert
            x.extend([i * 1.5 * t_block + t_block, i * 1.5 * t_block + 1.5 * t_block])
            y.extend([1, 1])
        
        fig.add_trace(go.Scatter(x=x, y=y, mode='lines', fill='tozeroy', name='√âtat'))
        fig.update_layout(
            xaxis_title="Temps (min)",
            yaxis=dict(tickvals=[0, 1], ticktext=['Ferm√©', 'Ouvert']),
            height=200
        )
        st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
    
    elif strategy == "Channels - Files s√©par√©es":
        st.markdown("""
        **Principe:** Une file d√©di√©e par population avec ressources allou√©es
        """)
        
        col1, col2, col3, col4 = st.columns(4)
        
        total_servers = n_servers
        
        with col1:
            servers_sup = st.number_input("Serveurs SUP", 1, total_servers, max(1, total_servers//4))
        with col2:
            servers_spe = st.number_input("Serveurs SP√â", 1, total_servers, max(1, total_servers//4))
        with col3:
            servers_ing = st.number_input("Serveurs ING1", 1, total_servers, max(1, total_servers//4))
        with col4:
            servers_admin = st.number_input("Serveurs Admin", 1, total_servers, max(1, total_servers//8))
        
        total_alloc = servers_sup + servers_spe + servers_ing + servers_admin
        st.info(f"Total allou√©: {total_alloc} serveurs (disponibles: {total_servers})")
    
    elif strategy == "Hybride - Pool avec priorit√©s":
        st.markdown("""
        **Principe:** Pool partag√© avec ordonnancement √† priorit√©s
        
        Les admins/staff ont la priorit√© la plus haute, suivis des ING1, puis des Pr√©pas.
        """)
        
        priorities = st.multiselect(
            "Ordre de priorit√© (haut ‚Üí bas)",
            ["Admin/Staff", "ING1", "Pr√©pa SP√â", "Pr√©pa SUP"],
            default=["Admin/Staff", "ING1", "Pr√©pa SP√â", "Pr√©pa SUP"]
        )
        
        if priorities:
            st.markdown("**Ordre de traitement:**")
            for i, p in enumerate(priorities, 1):
                st.write(f"{i}. {p}")
    
    st.divider()
    
    # Impact deadline
    st.subheader("‚è∞ Impact d'une Deadline")
    
    hours_to_deadline = st.slider("Heures avant deadline", 0.0, 48.0, 24.0, 0.5)
    
    impact_data = []
    for student_type, persona in personas.items():
        base_rate = persona.get_arrival_rate(14)
        deadline_rate = persona.get_arrival_rate(14, hours_to_deadline=hours_to_deadline)
        multiplier = deadline_rate / base_rate if base_rate > 0 else 1.0
        
        impact_data.append({
            'Population': persona.name,
            'Taux normal': base_rate,
            'Taux deadline': deadline_rate,
            'Multiplicateur': multiplier
        })
    
    df_impact = pd.DataFrame(impact_data)
    
    fig = go.Figure()
    fig.add_trace(go.Bar(x=df_impact['Population'], y=df_impact['Taux normal'], name='Normal'))
    fig.add_trace(go.Bar(x=df_impact['Population'], y=df_impact['Taux deadline'], name=f'Deadline {hours_to_deadline:.0f}h'))
    fig.update_layout(barmode='group', height=350, yaxis_title="Soumissions/heure")
    st.plotly_chart(apply_dark_theme(fig), use_container_width=True)


# ==============================================================================
# OPTIMISATION CO√õT / QUALIT√â DE SERVICE
# ==============================================================================

def render_optimization_tab(mu_rate: float, n_servers: int, buffer_size: int):
    """Onglet d'optimisation co√ªt/performance."""
    st.header("üí∞ Optimisation Co√ªt / Qualit√© de Service")
    
    st.markdown("""
    <div class="formula-box">
    <strong>Fonction objectif:</strong><br>
    min [ Œ± √ó E[T] + Œ≤ √ó Co√ªt(K, c, Œº) ] avec Œ± + Œ≤ = 1
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("""
    - **E[T]** = Temps moyen de s√©jour (temps d'attente + service)
    - **Co√ªt** = Co√ªt serveurs + Co√ªt rejets + Co√ªt insatisfaction
    - **Œ±** = Poids accord√© √† la performance
    - **Œ≤** = Poids accord√© au co√ªt
    """)
    
    st.divider()
    
    # Configuration
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("üìä Taux d'arriv√©e")
        
        input_mode = st.radio("Source", ["Manuel", "Personas"], horizontal=True)
        
        if input_mode == "Manuel":
            lambda_rate = st.number_input("Œª (soumissions/min)", 1.0, 200.0, 30.0, key="opt_lambda")
        else:
            personas = PersonaFactory.create_all_personas()
            selected = st.multiselect(
                "Populations actives",
                [p.name for p in personas.values()],
                default=[p.name for p in personas.values()]
            )
            
            lambda_rate = 0.0
            for p in personas.values():
                if p.name in selected:
                    lambda_rate += p.get_arrival_rate(14) / 60.0
            
            st.info(f"Œª combin√© = {lambda_rate:.2f} sub/min")
    
    with col2:
        st.subheader("‚öñÔ∏è Pond√©ration")
        
        alpha = st.slider("Œ± (poids co√ªt)", 0.0, 1.0, 0.7, 0.05)
        st.write(f"Œ≤ (poids temps) = {1-alpha:.2f}")
        
        st.markdown("---")
        st.subheader("üíµ Mod√®le de co√ªt")
        
        cost_server = st.number_input("Co√ªt serveur (‚Ç¨/h)", 0.1, 10.0, 0.50, 0.1)
        cost_reject = st.number_input("Co√ªt rejet (‚Ç¨)", 0.01, 1.0, 0.05, 0.01)
        cost_wait = st.number_input("P√©nalit√© attente (‚Ç¨/min)", 0.001, 0.1, 0.01, 0.001)
    
    st.divider()
    
    # Param√®tres de recherche
    st.subheader("üîç Espace de recherche")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        max_servers = st.slider("Max serveurs", 5, 30, 15)
    with col2:
        mu_min = st.number_input("Œº min", 5.0, 20.0, 5.0)
        mu_max = st.number_input("Œº max", 10.0, 50.0, 25.0)
    with col3:
        resolution = st.slider("R√©solution", 10, 40, 20)
    
    if st.button("üöÄ Lancer l'optimisation", type="primary"):
        run_optimization(lambda_rate, alpha, cost_server, cost_reject, cost_wait,
                        max_servers, mu_min, mu_max, resolution)


def run_optimization(lambda_rate, alpha, cost_server, cost_reject, cost_wait,
                    max_servers, mu_min, mu_max, resolution):
    """Ex√©cute l'optimisation et affiche les heatmaps."""
    
    with st.spinner("Optimisation en cours..."):
        server_range = np.arange(1, max_servers + 1)
        mu_range = np.linspace(mu_min, mu_max, resolution)
        
        Z_cost = np.zeros((len(mu_range), len(server_range)))
        Z_time = np.zeros((len(mu_range), len(server_range)))
        Z_score = np.zeros((len(mu_range), len(server_range)))
        
        for i, mu in enumerate(mu_range):
            for j, c in enumerate(server_range):
                rho = lambda_rate / (c * mu)
                
                if rho >= 1:
                    Z_cost[i, j] = np.nan
                    Z_time[i, j] = np.nan
                    Z_score[i, j] = np.nan
                else:
                    try:
                        queue = GenericQueue(lambda_rate, mu, f"M/M/{int(c)}", c=int(c))
                        metrics = queue.compute_theoretical_metrics()
                        
                        # Co√ªt horaire
                        server_cost = c * cost_server
                        wait_cost = metrics.Wq * cost_wait * lambda_rate * 60
                        total_cost = server_cost + wait_cost
                        
                        Z_cost[i, j] = total_cost
                        Z_time[i, j] = metrics.W
                        
                        # Score normalis√©
                        Z_score[i, j] = alpha * total_cost + (1 - alpha) * metrics.W
                        
                    except:
                        Z_cost[i, j] = np.nan
                        Z_time[i, j] = np.nan
                        Z_score[i, j] = np.nan
        
        # Normaliser le score pour affichage
        valid = ~np.isnan(Z_score)
        if valid.any():
            Z_score_norm = (Z_score - np.nanmin(Z_score)) / (np.nanmax(Z_score) - np.nanmin(Z_score))
        else:
            st.error("Aucune configuration stable trouv√©e")
            return
        
        # Afficher les heatmaps
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### üíµ Heatmap Co√ªt Total (‚Ç¨/h)")
            fig_cost = go.Figure(data=go.Heatmap(
                z=Z_cost,
                x=server_range,
                y=mu_range,
                colorscale='RdYlGn_r',
                colorbar=dict(title="‚Ç¨/h")
            ))
            fig_cost.update_layout(
                xaxis_title='Nombre de serveurs (c)',
                yaxis_title='Taux de service Œº',
                height=400
            )
            st.plotly_chart(apply_dark_theme(fig_cost), use_container_width=True)
        
        with col2:
            st.markdown("### ‚è±Ô∏è Heatmap Temps de S√©jour (min)")
            fig_time = go.Figure(data=go.Heatmap(
                z=Z_time,
                x=server_range,
                y=mu_range,
                colorscale='RdYlGn_r',
                colorbar=dict(title="min")
            ))
            fig_time.update_layout(
                xaxis_title='Nombre de serveurs (c)',
                yaxis_title='Taux de service Œº',
                height=400
            )
            st.plotly_chart(apply_dark_theme(fig_time), use_container_width=True)
        
        # Score combin√©
        st.markdown("### üéØ Heatmap Score Combin√© (Œ±√óCo√ªt + Œ≤√óTemps)")
        fig_score = go.Figure(data=go.Heatmap(
            z=Z_score_norm,
            x=server_range,
            y=mu_range,
            colorscale='RdYlGn_r',
            colorbar=dict(title="Score")
        ))
        fig_score.update_layout(
            xaxis_title='Nombre de serveurs (c)',
            yaxis_title='Taux de service Œº',
            height=400
        )
        st.plotly_chart(apply_dark_theme(fig_score), use_container_width=True)
        
        # Configuration optimale
        min_idx = np.nanargmin(Z_score)
        min_i, min_j = np.unravel_index(min_idx, Z_score.shape)
        
        opt_mu = mu_range[min_i]
        opt_c = server_range[min_j]
        opt_cost = Z_cost[min_i, min_j]
        opt_time = Z_time[min_i, min_j]
        
        st.success(f"""
        ### ‚úÖ Configuration Optimale
        
        | Param√®tre | Valeur |
        |-----------|--------|
        | **Serveurs (c)** | {opt_c} |
        | **Taux service (Œº)** | {opt_mu:.2f}/min |
        | **Co√ªt total** | {opt_cost:.2f} ‚Ç¨/h |
        | **Temps de s√©jour** | {opt_time:.3f} min |
        | **Utilisation (œÅ)** | {lambda_rate/(opt_c*opt_mu):.1%} |
        """)


# ==============================================================================
# AUTO-SCALING
# ==============================================================================

def render_autoscaling_tab(mu_rate1: float, mu_rate2: float, n_servers: int, K1: int, K2: int):
    """Onglet des strat√©gies d'auto-scaling."""
    st.header("üìà Strat√©gies d'Auto-Scaling")
    
    st.markdown("""
    <div class="scenario-box">
    <strong>Objectif:</strong> Adapter dynamiquement le nombre de serveurs √† la charge
    </div>
    """, unsafe_allow_html=True)
    
    # Types de scaling
    scaling_type = st.radio(
        "Strat√©gie de scaling",
        ["üîí Fixe", "üìÖ Programm√©", "‚ö° R√©actif", "üîÆ Pr√©dictif"],
        horizontal=True
    )
    
    st.divider()
    
    if scaling_type == "üîí Fixe":
        st.markdown("""
        **Scaling Fixe:** Nombre constant de serveurs
        
        ‚úÖ Simple √† g√©rer  
        ‚ùå Pas d'adaptation √† la charge
        """)
        
        st.metric("Serveurs fixes", n_servers)
    
    elif scaling_type == "üìÖ Programm√©":
        st.markdown("""
        **Scaling Programm√©:** Nombre de serveurs selon l'heure
        
        ‚úÖ Adapt√© aux patterns connus  
        ‚ùå Ne g√®re pas les pics impr√©vus
        """)
        
        st.subheader("Configuration horaire")
        
        schedule = {}
        cols = st.columns(6)
        for i, h in enumerate([0, 4, 8, 12, 16, 20]):
            with cols[i]:
                schedule[h] = st.number_input(f"{h}h-{h+4}h", 1, 20, n_servers, key=f"sched_{h}")
        
        # Visualisation
        hours = list(range(24))
        servers = []
        for h in hours:
            for start in sorted(schedule.keys(), reverse=True):
                if h >= start:
                    servers.append(schedule[start])
                    break
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=hours, y=servers, mode='lines+markers', fill='tozeroy', name='Serveurs'))
        fig.update_layout(xaxis_title="Heure", yaxis_title="Serveurs", height=300)
        st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
    
    elif scaling_type == "‚ö° R√©actif":
        st.markdown("""
        **Scaling R√©actif:** Ajustement bas√© sur la charge actuelle
        
        ‚úÖ R√©agit aux pics  
        ‚ùå Temps de r√©action (cooldown)
        """)
        
        col1, col2 = st.columns(2)
        with col1:
            scale_up = st.slider("Seuil scale-up (œÅ)", 0.5, 0.95, 0.8)
            scale_up_inc = st.number_input("Serveurs √† ajouter", 1, 5, 2)
        with col2:
            scale_down = st.slider("Seuil scale-down (œÅ)", 0.1, 0.5, 0.3)
            scale_down_inc = st.number_input("Serveurs √† retirer", 1, 3, 1)
        
        cooldown = st.slider("Cooldown (min)", 1, 30, 10)
        
        st.markdown(f"""
        **R√®gles:**
        - Si œÅ > {scale_up:.0%} ‚Üí +{scale_up_inc} serveurs
        - Si œÅ < {scale_down:.0%} ‚Üí -{scale_down_inc} serveur(s)
        - D√©lai entre ajustements: {cooldown} min
        """)
    
    elif scaling_type == "üîÆ Pr√©dictif":
        st.markdown("""
        **Scaling Pr√©dictif:** Anticipation bas√©e sur les donn√©es historiques
        
        ‚úÖ √âvite les temps de r√©action  
        ‚ùå N√©cessite des donn√©es historiques
        """)
        
        st.info("Le scaling pr√©dictif utilise les patterns de soumission historiques pour anticiper la charge.")
        
        # Simulation d'une pr√©diction
        st.subheader("Pr√©diction de charge (exemple)")
        
        hours = list(range(24))
        predicted_load = [20 + 30 * np.sin((h - 14) * np.pi / 12) ** 2 for h in hours]
        predicted_servers = [max(2, int(load / mu_rate1) + 1) for load in predicted_load]
        
        fig = make_subplots(specs=[[{"secondary_y": True}]])
        fig.add_trace(go.Scatter(x=hours, y=predicted_load, name="Charge pr√©dite", line=dict(color='blue')), secondary_y=False)
        fig.add_trace(go.Scatter(x=hours, y=predicted_servers, name="Serveurs recommand√©s", line=dict(color='green', dash='dash')), secondary_y=True)
        fig.update_layout(height=350)
        fig.update_yaxes(title_text="Charge (tags/min)", secondary_y=False)
        fig.update_yaxes(title_text="Serveurs", secondary_y=True)
        st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
    
    st.divider()
    
    # Comparaison des strat√©gies
    st.subheader("üìä Comparaison des strat√©gies")
    
    comparison_data = {
        'Strat√©gie': ['Fixe', 'Programm√©', 'R√©actif', 'Pr√©dictif'],
        'R√©activit√©': ['‚ùå Aucune', '‚ö†Ô∏è Limit√©e', '‚úÖ Bonne', '‚úÖ Excellente'],
        'Complexit√©': ['‚úÖ Simple', '‚ö†Ô∏è Moyenne', '‚ö†Ô∏è Moyenne', '‚ùå Complexe'],
        'Co√ªt': ['‚ö†Ô∏è Variable', '‚úÖ Optimis√©', '‚úÖ Optimis√©', '‚úÖ Tr√®s optimis√©'],
        'Pics impr√©vus': ['‚ùå Non g√©r√©', '‚ùå Non g√©r√©', '‚úÖ G√©r√©', '‚ö†Ô∏è Partiellement']
    }
    
    st.dataframe(pd.DataFrame(comparison_data), use_container_width=True)


# ==============================================================================
# BENCHMARK DES MOD√àLES
# ==============================================================================

def render_benchmark_tab(mu_rate: float, n_servers: int, buffer_size: int):
    """Onglet de benchmark comparatif des mod√®les de files."""
    st.header("üî¨ Benchmark des Mod√®les de Files d'Attente")
    
    col1, col2 = st.columns([2, 1])
    
    with col2:
        st.subheader("S√©lection des mod√®les")
        
        st.markdown("**Mono-serveur:**")
        show_mm1 = st.checkbox("M/M/1", value=True, key="b_mm1")
        show_md1 = st.checkbox("M/D/1", value=True, key="b_md1")
        show_mg1 = st.checkbox("M/G/1", value=True, key="b_mg1")
        
        st.markdown("**Multi-serveurs:**")
        show_mmc = st.checkbox("M/M/c", value=True, key="b_mmc")
        show_mdc = st.checkbox("M/D/c", value=True, key="b_mdc")
        show_mgc = st.checkbox("M/G/c", value=True, key="b_mgc")
        
        cv_squared = st.slider("CV¬≤ (pour M/G/*)", 0.0, 2.0, 1.0, 0.1)
    
    with col1:
        lambda_rate = st.slider("Œª - Taux d'arriv√©e", 1.0, 100.0, 30.0, key="b_lambda")
        mu_rate_b = st.slider("Œº - Taux de service", 1.0, 50.0, 10.0, key="b_mu")
        
        models_data = []
        
        # Mono-serveur
        if show_mm1 and lambda_rate < mu_rate_b:
            queue = GenericQueue(lambda_rate, mu_rate_b, "M/M/1")
            m = queue.compute_theoretical_metrics()
            models_data.append({'Mod√®le': 'M/M/1', 'L': m.L, 'Lq': m.Lq, 'W': m.W, 'Wq': m.Wq, 'œÅ': m.rho})
        
        if show_md1 and lambda_rate < mu_rate_b:
            queue = GenericQueue(lambda_rate, mu_rate_b, "M/D/1")
            m = queue.compute_theoretical_metrics()
            models_data.append({'Mod√®le': 'M/D/1', 'L': m.L, 'Lq': m.Lq, 'W': m.W, 'Wq': m.Wq, 'œÅ': m.rho})
        
        if show_mg1 and lambda_rate < mu_rate_b:
            queue = GenericQueue(lambda_rate, mu_rate_b, "M/G/1")
            queue.service_variance = cv_squared * (1/mu_rate_b)**2
            m = queue.compute_theoretical_metrics()
            models_data.append({'Mod√®le': f'M/G/1 (CV¬≤={cv_squared})', 'L': m.L, 'Lq': m.Lq, 'W': m.W, 'Wq': m.Wq, 'œÅ': m.rho})
        
        # Multi-serveurs
        if show_mmc and lambda_rate < n_servers * mu_rate_b:
            queue = GenericQueue(lambda_rate, mu_rate_b, f"M/M/{n_servers}", c=n_servers)
            m = queue.compute_theoretical_metrics()
            models_data.append({'Mod√®le': f'M/M/{n_servers}', 'L': m.L, 'Lq': m.Lq, 'W': m.W, 'Wq': m.Wq, 'œÅ': m.rho})
        
        if show_mdc and lambda_rate < n_servers * mu_rate_b:
            queue = GenericQueue(lambda_rate, mu_rate_b, f"M/D/{n_servers}", c=n_servers)
            m = queue.compute_theoretical_metrics()
            models_data.append({'Mod√®le': f'M/D/{n_servers}', 'L': m.L, 'Lq': m.Lq, 'W': m.W, 'Wq': m.Wq, 'œÅ': m.rho})
        
        if show_mgc and lambda_rate < n_servers * mu_rate_b:
            queue = GenericQueue(lambda_rate, mu_rate_b, f"M/G/{n_servers}", c=n_servers)
            queue.service_variance = cv_squared * (1/mu_rate_b)**2
            m = queue.compute_theoretical_metrics()
            models_data.append({'Mod√®le': f'M/G/{n_servers} (CV¬≤={cv_squared})', 'L': m.L, 'Lq': m.Lq, 'W': m.W, 'Wq': m.Wq, 'œÅ': m.rho})
        
        if models_data:
            df = pd.DataFrame(models_data)
            
            st.subheader("üìä Comparaison th√©orique")
            st.dataframe(df.style.format({
                'L': '{:.2f}',
                'Lq': '{:.2f}',
                'W': '{:.4f}',
                'Wq': '{:.4f}',
                'œÅ': '{:.2%}'
            }), use_container_width=True)
            
            # Graphiques
            fig = make_subplots(rows=1, cols=2, subplot_titles=['Temps d\'attente (Wq)', 'Longueur de queue (Lq)'])
            
            colors = px.colors.qualitative.Set2
            
            fig.add_trace(go.Bar(x=df['Mod√®le'], y=df['Wq'], marker_color=colors[:len(df)]), row=1, col=1)
            fig.add_trace(go.Bar(x=df['Mod√®le'], y=df['Lq'], marker_color=colors[:len(df)]), row=1, col=2)
            
            fig.update_layout(height=400, showlegend=False)
            st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
        else:
            st.warning("Aucun mod√®le stable s√©lectionn√©. V√©rifiez que Œª < c√óŒº")
    
    st.divider()
    
    # Simulation comparative
    st.subheader("üé≤ Simulation Monte Carlo Comparative")
    
    col_s1, col_s2 = st.columns([1, 3])
    
    with col_s1:
        sim_n = st.number_input("Clients", 100, 10000, 2000, key="b_sim_n")
        sim_runs = st.number_input("Runs", 1, 20, 5, key="b_sim_runs")
        run_benchmark = st.button("‚ñ∂Ô∏è Lancer benchmark", type="primary")
    
    with col_s2:
        if run_benchmark:
            run_model_benchmark(lambda_rate, mu_rate_b, n_servers, cv_squared, sim_n, sim_runs,
                               show_mm1, show_md1, show_mg1, show_mmc, show_mdc, show_mgc)


def run_model_benchmark(lambda_rate, mu_rate, n_servers, cv_squared, n_customers, n_runs,
                       show_mm1, show_md1, show_mg1, show_mmc, show_mdc, show_mgc):
    """Ex√©cute un benchmark Monte Carlo des mod√®les."""
    
    with st.spinner("Benchmark en cours..."):
        results = []
        
        models_to_test = []
        if show_mm1 and lambda_rate < mu_rate:
            models_to_test.append(("M/M/1", GenericQueue(lambda_rate, mu_rate, "M/M/1")))
        if show_md1 and lambda_rate < mu_rate:
            models_to_test.append(("M/D/1", GenericQueue(lambda_rate, mu_rate, "M/D/1")))
        if show_mg1 and lambda_rate < mu_rate:
            q = GenericQueue(lambda_rate, mu_rate, "M/G/1")
            q.service_variance = cv_squared * (1/mu_rate)**2
            models_to_test.append(("M/G/1", q))
        if show_mmc and lambda_rate < n_servers * mu_rate:
            models_to_test.append((f"M/M/{n_servers}", GenericQueue(lambda_rate, mu_rate, f"M/M/{n_servers}", c=n_servers)))
        if show_mdc and lambda_rate < n_servers * mu_rate:
            models_to_test.append((f"M/D/{n_servers}", GenericQueue(lambda_rate, mu_rate, f"M/D/{n_servers}", c=n_servers)))
        if show_mgc and lambda_rate < n_servers * mu_rate:
            q = GenericQueue(lambda_rate, mu_rate, f"M/G/{n_servers}", c=n_servers)
            q.service_variance = cv_squared * (1/mu_rate)**2
            models_to_test.append((f"M/G/{n_servers}", q))
        
        progress = st.progress(0)
        total = len(models_to_test) * n_runs
        current = 0
        
        for name, queue in models_to_test:
            for run in range(n_runs):
                try:
                    res = queue.simulate(n_customers=n_customers)
                    results.append({
                        'Mod√®le': name,
                        'Run': run + 1,
                        'Wq (sim)': np.mean(res.waiting_times) if len(res.waiting_times) > 0 else 0,
                        'W (sim)': np.mean(res.system_times) if len(res.system_times) > 0 else 0,
                        'Lq max': np.max(res.queue_length_trace) if len(res.queue_length_trace) > 0 else 0
                    })
                except Exception as e:
                    st.warning(f"{name}: {e}")
                
                current += 1
                progress.progress(current / total)
        
        progress.empty()
        
        if results:
            df = pd.DataFrame(results)
            
            # R√©sum√©
            summary = df.groupby('Mod√®le').agg({
                'Wq (sim)': ['mean', 'std'],
                'W (sim)': ['mean', 'std'],
                'Lq max': ['mean', 'std']
            }).round(4)
            
            st.markdown("### R√©sultats")
            
            # Cr√©er un DataFrame plus lisible
            summary_display = pd.DataFrame()
            for col in ['Wq (sim)', 'W (sim)', 'Lq max']:
                mean = summary[col]['mean']
                std = summary[col]['std']
                summary_display[col] = mean.astype(str) + ' ¬± ' + std.astype(str)
            
            st.dataframe(summary_display, use_container_width=True)
            
            # Box plots
            fig = make_subplots(rows=1, cols=2, subplot_titles=['Temps d\'attente', 'Temps de s√©jour'])
            
            for model in df['Mod√®le'].unique():
                model_data = df[df['Mod√®le'] == model]
                fig.add_trace(go.Box(y=model_data['Wq (sim)'], name=model), row=1, col=1)
                fig.add_trace(go.Box(y=model_data['W (sim)'], name=model, showlegend=False), row=1, col=2)
            
            fig.update_layout(height=400)
            st.plotly_chart(apply_dark_theme(fig), use_container_width=True)


if __name__ == "__main__":
    main()

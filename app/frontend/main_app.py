"""
Application Streamlit pour la simulation de la moulinette EPITA.

Interface interactive permettant de:
- Visualiser le mod√®le Waterfall (files infinies/finies)
- Simuler les m√©canismes de backup et leur impact
- Analyser les populations diff√©renci√©es (Channels & Dams)
- Optimiser co√ªt/qualit√© de service
- Obtenir des recommandations de scaling

Bas√© sur le rapport de projet ERO2 - Janvier 2026

Lancer avec: streamlit run app/frontend/main_app.py
"""

import streamlit as st
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import sys
from pathlib import Path
from typing import Callable, Dict, List, Tuple, Optional
import math
import heapq

# Ajouter le chemin parent pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from app.models.base_queue import GenericQueue, ChainQueue, QueueMetrics, SimulationResults
from app.personas import PersonaFactory, StudentType
from app.personas.usage_patterns import AcademicPeriod
from app.simulation import RushSimulator, MoulinetteSystem, SimulationConfig, ServerConfig
from app.optimization import CostOptimizer, ScalingAdvisor, CostModel, ScalingPolicy
import json
from datetime import datetime
import os


def apply_dark_theme(fig):
    """Applique un th√®me sombre aux graphiques Plotly pour am√©liorer la lisibilit√©."""
    fig.update_layout(
        plot_bgcolor='#1e1e1e',
        paper_bgcolor='#2d2d2d',
        font=dict(color='#e0e0e0'),
        xaxis=dict(
            gridcolor='#404040',
            zerolinecolor='#404040'
        ),
        yaxis=dict(
            gridcolor='#404040',
            zerolinecolor='#404040'
        ),
        legend=dict(
            bgcolor='rgba(45, 45, 45, 0.8)',
            bordercolor='#404040',
            borderwidth=1
        )
    )
    return fig


def run_app():
    """Point d'entr√©e principal."""
    main()


def main():
    """Application principale."""
    st.set_page_config(
        page_title="Moulinette Simulator - EPITA",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # CSS personnalis√©
    st.markdown("""
    <style>
    .main-header {
        font-size: 5.5rem;
        font-weight: 900;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        text-align: center;
        margin-bottom: 1rem;
        margin-top: 2rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
        letter-spacing: 2px;
        animation: fadeInDown 1s ease-out;
    }
    @keyframes fadeInDown {
        from {
            opacity: 0;
            transform: translateY(-30px);
        }
        to {
            opacity: 1;
            transform: translateY(0);
        }
    }
    .sub-header {
        font-size: 1.4rem;
        color: #888;
        text-align: center;
        margin-bottom: 3rem;
        font-weight: 300;
        letter-spacing: 1px;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
    }
    .scenario-box {
        background: #2b3e50;
        border-left: 4px solid #1f77b4;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 8px 8px 0;
        color: #ecf0f1;
    }
    .formula-box {
        background: #34495e;
        border: 1px solid #3498db;
        padding: 1rem;
        border-radius: 8px;
        font-family: 'Courier New', monospace;
        text-align: center;
        margin: 1rem 0;
        color: #ecf0f1;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 1rem;
    }
    </style>
    """, unsafe_allow_html=True)
    
    st.markdown('<p class="main-header">Moulinette Simulator</p>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">Optimisation des files d\'attente pour la moulinette EPITA - Projet ERO2</p>', unsafe_allow_html=True)
    
    # Sidebar pour les param√®tres globaux
    with st.sidebar:
        st.header("‚öôÔ∏è Param√®tres Globaux")
        st.markdown("---")
        
        st.subheader("File 1: Ex√©cution Tests")
        mu_rate1 = st.slider(
            "Œº‚ÇÅ - Taux de service (tests/min)",
            min_value=1.0, max_value=50.0, value=10.0, step=0.5,
            help="Vitesse de traitement des test-suites par serveur"
        )
        n_servers = st.slider(
            "c - Nombre de runners",
            min_value=1, max_value=20, value=4
        )
        K1 = st.slider(
            "K‚ÇÅ - Capacit√© buffer File 1",
            min_value=10, max_value=1000, value=100, step=10
        )
        
        st.markdown("---")
        st.subheader("File 2: Renvoi R√©sultats")
        mu_rate2 = st.slider(
            "Œº‚ÇÇ - Taux de service (r√©sultats/min)",
            min_value=1.0, max_value=100.0, value=20.0, step=1.0,
            help="Vitesse d'envoi des r√©sultats au frontend"
        )
        K2 = st.slider(
            "K‚ÇÇ - Capacit√© buffer File 2",
            min_value=10, max_value=500, value=50, step=10
        )
        
        st.markdown("---")
        st.subheader("Mod√®le File 2")
        file2_model = st.radio(
            "Type de service",
            ["M/M/1 (Exponentiel)", "M/D/1 (D√©terministe)"],
            help="M/D/1 r√©duit le temps d'attente de 50% vs M/M/1"
        )
    
    # Onglets principaux
    tabs = st.tabs([
        "Sc√©nario 1: Waterfall",
        "Backup",
        "Sc√©nario 2: Channels & Dams",
        "Optimisation Co√ªt/QoS",
        "Auto-Scaling"
    ])
    
    with tabs[0]:
        render_waterfall_scenario(mu_rate1, mu_rate2, n_servers, K1, K2, file2_model)
    
    with tabs[1]:
        render_backup_scenario(mu_rate1, mu_rate2, n_servers, K1, K2)
    
    with tabs[2]:
        render_channels_dams_tab(mu_rate1, n_servers, K1)
    
    with tabs[3]:
        render_optimization_tab(mu_rate1, n_servers, K1)
    
    with tabs[4]:
        render_autoscaling_tab(mu_rate1, mu_rate2, n_servers, K1, K2)

# ==============================================================================
# SC√âNARIO 1: MOD√àLE WATERFALL
# ==============================================================================

def render_waterfall_scenario(mu_rate1: float, mu_rate2: float, n_servers: int, K1: int, K2: int, file2_model: str):
    """Sc√©nario 1: Mod√®le Waterfall avec files infinies puis finies."""
    st.header("Sc√©nario 1: Mod√®le Waterfall")
    
    st.markdown("""
    <div class="scenario-box">
    <strong>Architecture en cascade de la moulinette:</strong><br>
    <code>√âtudiants ‚Üí Buffer‚ÇÅ (K‚ÇÅ) ‚Üí Runners (c) ‚Üí Buffer‚ÇÇ (K‚ÇÇ) ‚Üí Frontend (1 serveur)</code><br><br>
    <strong>Mod√®le th√©orique:</strong> Cha√Æne de files M/M/c/K‚ÇÅ ‚Üí M/M/1/K‚ÇÇ (ou M/D/1/K‚ÇÇ)
    </div>
    """, unsafe_allow_html=True)

    
    # Configuration de la simulation
    st.subheader("Configuration de la Simulation")
    
    col_config1, col_config2, col_config3 = st.columns(3)
    
    with col_config1:
        lambda_rate = st.slider(
            "Œª - Taux d'arriv√©e (tags/min)",
            min_value=1.0, max_value=100.0, value=30.0, step=1.0,
            key="waterfall_lambda",
            help="Nombre de tags soumis par minute"
        )
    
    with col_config2:
        capacity_mode = st.radio(
            "Mode de capacit√©",
            ["Files infinies (K=‚àû)", "Files finies (K limit√©)"],
            key="waterfall_capacity",
            help="Files infinies: th√©orie classique. Files finies: mod√®le r√©aliste avec rejets."
        )
    
    with col_config3:
        use_md1 = "M/D/1" in file2_model
        st.metric("Type File 2", "M/D/1 (D√©terministe)" if use_md1 else "M/M/1 (Exponentiel)")
        if use_md1:
            st.info("‚úì Service d√©terministe: -50% temps d'attente")
    
    st.divider()
    
    # Section analyse th√©orique
    st.subheader("Analyse Th√©orique des Files")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### File 1: Ex√©cution des tests")
        st.markdown(f"<div style='background:#2d4059;padding:1rem;border-radius:8px;margin-bottom:1rem;'>"
                   f"<strong>Mod√®le:</strong> M/M/{n_servers}{'/' + str(K1) if capacity_mode == 'Files finies (K limit√©)' else ''}<br>"
                   f"<strong>Serveurs:</strong> {n_servers} runners parall√®les<br>"
                   f"<strong>Service:</strong> Œº‚ÇÅ = {mu_rate1} tests/min/runner"
                   f"</div>", unsafe_allow_html=True)
        
        # Condition de stabilit√©
        rho1 = lambda_rate / (n_servers * mu_rate1)
        stable1 = rho1 < 1 or capacity_mode == "Files finies (K limit√©)"
        
        st.markdown(f"""
        <div class="formula-box">
        <strong>Condition de stabilit√©:</strong><br>
        œÅ‚ÇÅ = Œª/(c√óŒº‚ÇÅ) = {lambda_rate:.1f}/({n_servers}√ó{mu_rate1:.1f}) = <strong>{rho1:.3f}</strong>
        </div>
        """, unsafe_allow_html=True)
        
        if rho1 < 1:
            st.success(f"‚úì Syst√®me stable (œÅ‚ÇÅ = {rho1:.2%} < 100%)")
        else:
            if capacity_mode == "Files finies (K limit√©)":
                st.warning(f"œÅ‚ÇÅ = {rho1:.2%} ‚â• 100% mais stable gr√¢ce au buffer fini K‚ÇÅ={K1}")
            else:
                st.error(f"‚úó Syst√®me instable (œÅ‚ÇÅ = {rho1:.2%} ‚â• 100%)")
        
        if stable1 or capacity_mode == "Files finies (K limit√©)":
            try:
                if capacity_mode == "Files infinies (K=‚àû)":
                    queue1 = GenericQueue(lambda_rate, mu_rate1, f"M/M/{n_servers}", c=n_servers)
                    metrics1 = queue1.compute_theoretical_metrics()
                    P_K1 = 0.0
                    lambda_eff = lambda_rate
                else:
                    # M/M/c/K - calcul avec file finie
                    metrics1, P_K1 = compute_mmck_metrics(lambda_rate, mu_rate1, n_servers, K1)
                    lambda_eff = lambda_rate * (1 - P_K1)
                
                # Afficher les m√©triques
                display_queue_metrics(metrics1, "File 1", P_K1)
                
            except Exception as e:
                st.error(f"Erreur de calcul: {e}")
                metrics1 = None
                lambda_eff = 0
        else:
            metrics1 = None
            lambda_eff = 0
    
    with col2:
        st.subheader(f"File 2: Renvoi des r√©sultats ({'M/D/1' if use_md1 else 'M/M/1'})")
        
        if lambda_eff > 0:
            # Le taux d'entr√©e en file 2 = taux de sortie de file 1
            lambda2 = lambda_eff
            rho2 = lambda2 / mu_rate2
            
            st.markdown(f"""
            **Taux d'arriv√©e effectif:** Œª‚ÇÇ = Œª_eff = {lambda2:.2f} tags/min
            
            **Condition de stabilit√©:** œÅ‚ÇÇ = Œª‚ÇÇ/Œº‚ÇÇ = {lambda2:.2f}/{mu_rate2:.1f} = **{rho2:.3f}**
            """)
            
            if rho2 < 1:
                st.success(f"Syst√®me stable (œÅ‚ÇÇ = {rho2:.2%} < 100%)")
            else:
                if capacity_mode == "Files finies (K limit√©)":
                    st.warning(f"œÅ‚ÇÇ = {rho2:.2%} ‚â• 100% mais stable gr√¢ce au buffer fini K‚ÇÇ={K2}")
                else:
                    st.error(f"Syst√®me instable (œÅ‚ÇÇ = {rho2:.2%} ‚â• 100%)")
            
            try:
                if capacity_mode == "Files infinies (K=‚àû)" and rho2 < 1:
                    notation = "M/D/1" if use_md1 else "M/M/1"
                    queue2 = GenericQueue(lambda2, mu_rate2, notation)
                    metrics2 = queue2.compute_theoretical_metrics()
                    P_K2 = 0.0
                else:
                    # M/M/1/K ou M/D/1/K
                    metrics2, P_K2 = compute_mm1k_metrics(lambda2, mu_rate2, K2, use_md1)
                
                display_queue_metrics(metrics2, "File 2", P_K2)
                
                # Avantage M/D/1 vs M/M/1
                if use_md1:
                    st.info("**M/D/1:** Temps d'attente r√©duit de ~50% gr√¢ce au service d√©terministe")
                
            except Exception as e:
                st.error(f"Erreur de calcul: {e}")
                metrics2 = None
                P_K2 = 0
        else:
            st.warning("Calculez d'abord la File 1")
            metrics2 = None
            P_K2 = 0
    
    st.divider()
    
    st.subheader("Simulation Monte Carlo")
    
    col_sim1, col_sim2 = st.columns([1, 2])
    
    with col_sim1:
        n_customers = st.number_input("Nombre de clients", 100, 10000, 2000, step=100, key="waterfall_n")
        n_runs = st.number_input("Nombre de r√©p√©titions", 1, 20, 5, key="waterfall_runs")
        run_sim = st.button("Lancer simulation Waterfall", type="primary", key="waterfall_run")
    
    with col_sim2:
        if run_sim:
            run_waterfall_simulation(lambda_rate, mu_rate1, mu_rate2, n_servers, K1, K2, 
                                    n_customers, n_runs, capacity_mode, use_md1)


def create_waterfall_diagram(lambda_rate, mu_rate1, mu_rate2, n_servers, K1, K2, capacity_mode):
    """Cr√©e un sch√©ma visuel du syst√®me Waterfall."""
    fig = go.Figure()
    
    # Param√®tres de positionnement
    y_base = 0.5
    
    # Source (√©tudiants)
    fig.add_trace(go.Scatter(
        x=[0], y=[y_base],
        mode='markers',
        marker=dict(size=40, color='#2ecc71', symbol='circle'),
        name='√âtudiants', showlegend=False
    ))
    fig.add_annotation(x=0, y=y_base-0.15, text=f"Œª={lambda_rate}", showarrow=False)
    
    # Fl√®che vers Buffer 1
    fig.add_annotation(x=0.5, y=y_base, ax=0.15, ay=y_base,
                      xref="x", yref="y", axref="x", ayref="y",
                      showarrow=True, arrowhead=2, arrowsize=1.5)
    
    # Buffer 1
    k1_text = "‚àû" if "infinies" in capacity_mode else str(K1)
    fig.add_trace(go.Scatter(
        x=[1], y=[y_base],
        mode='markers',
        marker=dict(size=50, color='#3498db', symbol='square'),
        name='Buffer 1', showlegend=False
    ))
    fig.add_annotation(x=1, y=y_base-0.15, text=f"K‚ÇÅ={k1_text}", showarrow=False)
    
    # Fl√®che vers Runners
    fig.add_annotation(x=1.5, y=y_base, ax=1.15, ay=y_base,
                      xref="x", yref="y", axref="x", ayref="y",
                      showarrow=True, arrowhead=2, arrowsize=1.5)
    
    # Runners (multi-serveurs)
    fig.add_trace(go.Scatter(
        x=[2], y=[y_base],
        mode='markers',
        marker=dict(size=60, color='#e74c3c', symbol='square'),
        name='Runners', showlegend=False
    ))
    fig.add_annotation(x=2, y=y_base-0.15, text=f"c={n_servers}, Œº‚ÇÅ={mu_rate1}", showarrow=False)
    
    # Fl√®che vers Buffer 2
    fig.add_annotation(x=2.5, y=y_base, ax=2.15, ay=y_base,
                      xref="x", yref="y", axref="x", ayref="y",
                      showarrow=True, arrowhead=2, arrowsize=1.5)
    
    # Buffer 2
    k2_text = "‚àû" if "infinies" in capacity_mode else str(K2)
    fig.add_trace(go.Scatter(
        x=[3], y=[y_base],
        mode='markers',
        marker=dict(size=50, color='#9b59b6', symbol='square'),
        name='Buffer 2', showlegend=False
    ))
    fig.add_annotation(x=3, y=y_base-0.15, text=f"K‚ÇÇ={k2_text}", showarrow=False)
    
    # Fl√®che vers Frontend
    fig.add_annotation(x=3.5, y=y_base, ax=3.15, ay=y_base,
                      xref="x", yref="y", axref="x", ayref="y",
                      showarrow=True, arrowhead=2, arrowsize=1.5)
    
    # Frontend
    fig.add_trace(go.Scatter(
        x=[4], y=[y_base],
        mode='markers',
        marker=dict(size=50, color='#f39c12', symbol='circle'),
        name='Frontend', showlegend=False
    ))
    fig.add_annotation(x=4, y=y_base-0.15, text=f"Œº‚ÇÇ={mu_rate2}", showarrow=False)
    
    fig.update_layout(
        height=200,
        xaxis=dict(showgrid=False, showticklabels=False, range=[-0.5, 4.5]),
        yaxis=dict(showgrid=False, showticklabels=False, range=[0, 1]),
        margin=dict(l=20, r=20, t=30, b=20),
        title="Architecture du syst√®me Waterfall"
    )
    
    return apply_dark_theme(fig)


def display_queue_metrics(metrics: QueueMetrics, name: str, P_K: float = 0.0):
    """Affiche les m√©triques d'une file."""
    col1, col2 = st.columns(2)
    with col1:
        st.metric(f"L (clients dans syst√®me)", f"{metrics.L:.2f}")
        st.metric(f"W (temps de s√©jour)", f"{metrics.W:.3f} min")
    with col2:
        st.metric(f"Lq (en attente)", f"{metrics.Lq:.2f}")
        st.metric(f"Wq (temps d'attente)", f"{metrics.Wq:.3f} min")
    
    if P_K > 0:
        st.metric(f"P(blocage)", f"{P_K:.4%}")


def compute_mmck_metrics(lambda_rate: float, mu_rate: float, c: int, K: int) -> Tuple[QueueMetrics, float]:
    """Calcule les m√©triques pour M/M/c/K."""
    rho = lambda_rate / (c * mu_rate)
    a = lambda_rate / mu_rate
    
    # Calcul de œÄ‚ÇÄ
    sum1 = sum((a ** n) / math.factorial(n) for n in range(c))
    if rho != 1:
        sum2 = ((a ** c) / math.factorial(c)) * (1 - rho ** (K - c + 1)) / (1 - rho)
    else:
        sum2 = ((a ** c) / math.factorial(c)) * (K - c + 1)
    
    P0 = 1 / (sum1 + sum2)
    
    # Calcul de œÄ_K (probabilit√© de blocage)
    if rho != 1:
        P_K = ((a ** K) / (math.factorial(c) * (c ** (K - c)))) * P0
    else:
        P_K = ((a ** K) / math.factorial(K)) * P0
    
    # Lambda effectif
    lambda_eff = lambda_rate * (1 - P_K)
    
    # Calcul de L
    L = 0
    for n in range(1, c + 1):
        L += n * ((a ** n) / math.factorial(n)) * P0
    
    if rho != 1:
        numerator = (a ** c * rho * P0) / math.factorial(c)
        L += numerator * (1 - rho ** (K - c + 1) - (1 - rho) * (K - c + 1) * rho ** (K - c)) / ((1 - rho) ** 2)
    
    # M√©triques via Little
    Lq = max(0, L - (1 - P0) * c / c)  # Approximation
    W = L / lambda_eff if lambda_eff > 0 else 0
    Wq = Lq / lambda_eff if lambda_eff > 0 else 0
    
    metrics = QueueMetrics(
        rho=rho if rho < 1 else 1.0,
        L=L,
        Lq=Lq,
        W=W,
        Wq=Wq,
        Ws=1/mu_rate,
        P0=P0,
        Pk=P_K,
        lambda_eff=lambda_eff,
        throughput=lambda_eff
    )
    
    return metrics, P_K


def compute_mm1k_metrics(lambda_rate: float, mu_rate: float, K: int, deterministic: bool = False) -> Tuple[QueueMetrics, float]:
    """Calcule les m√©triques pour M/M/1/K ou M/D/1/K."""
    rho = lambda_rate / mu_rate
    
    if rho == 1:
        P0 = 1 / (K + 1)
        P_K = 1 / (K + 1)
        L = K / 2
    else:
        P0 = (1 - rho) / (1 - rho ** (K + 1))
        P_K = P0 * (rho ** K)
        L = rho * (1 - (K + 1) * rho ** K + K * rho ** (K + 1)) / ((1 - rho) * (1 - rho ** (K + 1)))
    
    lambda_eff = lambda_rate * (1 - P_K)
    
    W = L / lambda_eff if lambda_eff > 0 else 0
    Wq = W - 1/mu_rate
    Lq = lambda_eff * Wq if Wq > 0 else 0
    
    # Pour M/D/1: r√©duction de 50% du temps d'attente
    if deterministic:
        Wq = Wq / 2
        Lq = Lq / 2
        W = Wq + 1/mu_rate
    
    metrics = QueueMetrics(
        rho=min(rho, 1.0),
        L=L,
        Lq=Lq,
        W=W,
        Wq=Wq,
        Ws=1/mu_rate,
        P0=P0,
        Pk=P_K,
        lambda_eff=lambda_eff,
        throughput=lambda_eff
    )
    
    return metrics, P_K


def run_waterfall_simulation(lambda_rate, mu_rate1, mu_rate2, n_servers, K1, K2, 
                             n_customers, n_runs, capacity_mode, use_md1):
    """Ex√©cute une simulation Monte Carlo du syst√®me Waterfall."""
    with st.spinner("Simulation en cours..."):
        results_all = []
        
        for run in range(n_runs):
            # File 1: M/M/c ou M/M/c/K
            if "infinies" in capacity_mode:
                queue1 = GenericQueue(lambda_rate, mu_rate1, f"M/M/{n_servers}", c=n_servers)
            else:
                queue1 = GenericQueue(lambda_rate, mu_rate1, f"M/M/{n_servers}", c=n_servers, K=K1)
            
            res1 = queue1.simulate(n_customers=n_customers)
            
            # File 2: utilise les temps de d√©part de File 1 comme arriv√©es
            if len(res1.departure_times) > 0:
                notation2 = "M/D/1" if use_md1 else "M/M/1"
                if "infinies" in capacity_mode:
                    queue2 = GenericQueue(lambda_rate, mu_rate2, notation2)
                else:
                    queue2 = GenericQueue(lambda_rate, mu_rate2, notation2, K=K2)
                
                res2 = queue2.simulate(external_arrival_times=res1.departure_times)
                
                results_all.append({
                    'run': run + 1,
                    'file1_served': res1.n_served,
                    'file1_rejected': res1.n_rejected,
                    'file1_wait': np.mean(res1.waiting_times) if len(res1.waiting_times) > 0 else 0,
                    'file1_system': np.mean(res1.system_times) if len(res1.system_times) > 0 else 0,
                    'file2_served': res2.n_served,
                    'file2_rejected': res2.n_rejected,
                    'file2_wait': np.mean(res2.waiting_times) if len(res2.waiting_times) > 0 else 0,
                    'file2_system': np.mean(res2.system_times) if len(res2.system_times) > 0 else 0,
                    'total_wait': (np.mean(res1.waiting_times) if len(res1.waiting_times) > 0 else 0) + 
                                  (np.mean(res2.waiting_times) if len(res2.waiting_times) > 0 else 0),
                    'total_system': (np.mean(res1.system_times) if len(res1.system_times) > 0 else 0) + 
                                    (np.mean(res2.system_times) if len(res2.system_times) > 0 else 0),
                })
        
        if results_all:
            df = pd.DataFrame(results_all)
            
            # Statistiques
            st.markdown("### R√©sultats de simulation")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Temps d'attente moyen total", 
                         f"{df['total_wait'].mean():.3f} ¬± {df['total_wait'].std():.3f} min")
            with col2:
                st.metric("Temps de s√©jour moyen total",
                         f"{df['total_system'].mean():.3f} ¬± {df['total_system'].std():.3f} min")
            with col3:
                reject_rate = df['file1_rejected'].sum() / (df['file1_served'].sum() + df['file1_rejected'].sum())
                st.metric("Taux de rejet File 1", f"{reject_rate:.2%}")
            
            # Graphique
            fig = make_subplots(rows=1, subplot_titles=['Temps par file'])
            
            fig.add_trace(go.Box(y=df['file1_wait'], name='Attente F1'), row=1, col=1)
            fig.add_trace(go.Box(y=df['file2_wait'], name='Attente F2'), row=1, col=1)
            fig.add_trace(go.Box(y=df['file1_system'], name='S√©jour F1'), row=1, col=1)
            fig.add_trace(go.Box(y=df['file2_system'], name='S√©jour F2'), row=1, col=1)
                        
            fig.update_layout(height=400, showlegend=True)
            st.plotly_chart(apply_dark_theme(fig), use_container_width=True)


# ==============================================================================
# SC√âNARIO 2: M√âCANISMES DE BACKUP
# ==============================================================================

def simulate_waterfall_with_backup(
    lambda_rate: float,
    mu_rate1: float,
    mu_rate2: float,
    mu_backup: float,
    n_servers: int,
    K1: int,
    K2: int,
    n_customers: int,
    p_backup: float = 1.0,
    seed: Optional[int] = None
) -> dict:
    """
    Simule le syst√®me Waterfall avec m√©canisme de backup entre les deux files.
    
    Architecture:
    File 1 (M/M/c/K1) ‚Üí [BACKUP] ‚Üí File 2 (M/M/1/K2) ‚Üí Frontend
    
    Le backup sauvegarde les r√©sultats AVANT l'envoi vers la file 2.
    Si la file 2 rejette (pleine), les donn√©es sont r√©cup√©r√©es depuis le backup.
    
    Args:
        lambda_rate: Taux d'arriv√©e des tags
        mu_rate1: Taux de service de la file 1 (moulinette)
        mu_rate2: Taux de service de la file 2 (envoi r√©sultats)
        mu_backup: Taux de sauvegarde (backup/min)
        n_servers: Nombre de serveurs dans la file 1
        K1: Capacit√© de la file 1
        K2: Capacit√© de la file 2
        n_customers: Nombre de clients √† simuler
        p_backup: Probabilit√© de faire un backup (0 √† 1)
        seed: Graine al√©atoire
    
    Returns:
        dict avec les m√©triques de simulation
    """
    rng = np.random.default_rng(seed)
    
    # G√©n√©ration des arriv√©es
    interarrival_times = rng.exponential(1/lambda_rate, n_customers)
    arrival_times = np.cumsum(interarrival_times)
    
    # Structures pour File 1
    queue1 = []
    busy_servers1 = 0
    event_heap = []
    
    # Structures pour File 2
    queue2 = []
    busy_server2 = False
    
    # Backup storage
    backup_storage = {}  # client_id -> (result_data, backup_time)
    
    # Compteurs
    n_rejected_f1 = 0
    n_rejected_f2 = 0
    n_recovered_from_backup = 0
    n_backed_up = 0
    
    # Ensemble pour tracker les clients d√©j√† rejet√©s de File 2 (une seule fois par client)
    rejected_f2_clients = set()
    
    # M√©triques temporelles
    file1_wait_times = []
    file1_system_times = []
    file2_wait_times = []
    file2_system_times = []
    backup_times = []
    total_system_times = []
    
    # Service times (pr√©-g√©n√©r√©s)
    service_times_f1 = rng.exponential(1/mu_rate1, n_customers)
    service_times_f2 = rng.exponential(1/mu_rate2, n_customers)
    backup_service_times = rng.exponential(1/mu_backup, n_customers)
    
    # D√©cisions de backup (pr√©-g√©n√©r√©es)
    do_backup = rng.random(n_customers) < p_backup
    
    # Initialiser les arriv√©es
    for i, t in enumerate(arrival_times):
        heapq.heappush(event_heap, (t, "arrival_f1", i))
    
    # Compteur syst√®me
    system_size_f1 = 0
    system_size_f2 = 0
    
    # Traces temporelles
    time_trace = []
    f1_size_trace = []
    f2_size_trace = []
    backup_size_trace = []
    
    # Temps d'entr√©e dans le syst√®me pour chaque client
    client_arrival_f1 = {}
    client_exit_f1 = {}
    client_arrival_f2 = {}
    client_completed = set()
    
    while event_heap:
        time, event_type, client_id = heapq.heappop(event_heap)
        
        if event_type == "arrival_f1":
            # Arriv√©e dans File 1
            if system_size_f1 >= K1:
                n_rejected_f1 += 1
                continue
            
            system_size_f1 += 1
            client_arrival_f1[client_id] = time
            
            if busy_servers1 < n_servers:
                # Service imm√©diat
                busy_servers1 += 1
                depart_time = time + service_times_f1[client_id]
                heapq.heappush(event_heap, (depart_time, "depart_f1", client_id))
                file1_wait_times.append(0.0)
            else:
                queue1.append(client_id)
        
        elif event_type == "depart_f1":
            # D√©part de File 1 ‚Üí Backup ‚Üí File 2
            system_size_f1 -= 1
            busy_servers1 -= 1
            
            client_exit_f1[client_id] = time
            file1_system_times.append(time - client_arrival_f1[client_id])
            
            # Faire le backup AVANT d'envoyer √† la file 2
            if do_backup[client_id]:
                backup_time = backup_service_times[client_id]
                backup_storage[client_id] = {
                    'backup_time': time,
                    'data_ready': time + backup_time
                }
                backup_times.append(backup_time)
                n_backed_up += 1
                # Le backup est asynchrone, on continue vers File 2 imm√©diatement
                # mais les donn√©es sont sauvegard√©es
            
            # Essayer d'entrer dans File 2
            heapq.heappush(event_heap, (time, "try_enter_f2", client_id))
            
            # Servir le prochain dans File 1
            if queue1:
                next_client = queue1.pop(0)
                busy_servers1 += 1
                wait_time = time - client_arrival_f1[next_client]
                file1_wait_times.append(wait_time)
                depart_time = time + service_times_f1[next_client]
                heapq.heappush(event_heap, (depart_time, "depart_f1", next_client))
        
        elif event_type == "try_enter_f2":
            # Tentative d'entr√©e dans File 2
            if system_size_f2 >= K2:
                # File 2 pleine !
                # Compter le rejet seulement si c'est la premi√®re fois pour ce client
                if client_id not in rejected_f2_clients:
                    print(rejected_f2_clients)
                    n_rejected_f2 += 1
                    rejected_f2_clients.add(client_id)
                
                # R√©cup√©ration depuis le backup si disponible
                if client_id in backup_storage:
                    n_recovered_from_backup += 1
                    # On planifie une r√©-tentative apr√®s un d√©lai
                    retry_time = time + rng.exponential(1/mu_rate2)  # Attendre avant retry
                    heapq.heappush(event_heap, (retry_time, "retry_from_backup", client_id))
                # Sinon: page blanche (donn√©es perdues)
                continue
            
            system_size_f2 += 1
            client_arrival_f2[client_id] = time
            
            if not busy_server2:
                # Service imm√©diat
                busy_server2 = True
                depart_time = time + service_times_f2[client_id]
                heapq.heappush(event_heap, (depart_time, "depart_f2", client_id))
                file2_wait_times.append(0.0)
            else:
                queue2.append(client_id)
        
        elif event_type == "retry_from_backup":
            # R√©-tentative depuis le backup
            if system_size_f2 >= K2:
                # Toujours pleine, on re-planifie
                retry_time = time + rng.exponential(1/mu_rate2)
                heapq.heappush(event_heap, (retry_time, "retry_from_backup", client_id))
                continue
            
            system_size_f2 += 1
            client_arrival_f2[client_id] = time
            
            if not busy_server2:
                busy_server2 = True
                depart_time = time + service_times_f2[client_id]
                heapq.heappush(event_heap, (depart_time, "depart_f2", client_id))
                file2_wait_times.append(0.0)
            else:
                queue2.append(client_id)
        
        elif event_type == "depart_f2":
            # D√©part de File 2 ‚Üí Client compl√©t√©
            system_size_f2 -= 1
            busy_server2 = False
            
            if client_id in client_arrival_f2:
                file2_system_times.append(time - client_arrival_f2[client_id])
            
            if client_id in client_arrival_f1:
                total_system_times.append(time - client_arrival_f1[client_id])
            
            client_completed.add(client_id)
            
            # Nettoyer le backup
            if client_id in backup_storage:
                del backup_storage[client_id]
            
            # Servir le prochain dans File 2
            if queue2:
                next_client = queue2.pop(0)
                busy_server2 = True
                wait_time = time - client_arrival_f2[next_client]
                file2_wait_times.append(wait_time)
                depart_time = time + service_times_f2[next_client]
                heapq.heappush(event_heap, (depart_time, "depart_f2", next_client))
        
        # Mise √† jour des traces
        time_trace.append(time)
        f1_size_trace.append(system_size_f1)
        f2_size_trace.append(system_size_f2)
        backup_size_trace.append(len(backup_storage))
    
    # Calcul des m√©triques finales
    n_completed = len(client_completed)
    n_lost = n_rejected_f2 - n_recovered_from_backup  # Vraies pages blanches
    
    return {
        'n_arrivals': n_customers,
        'n_rejected_f1': n_rejected_f1,
        'n_rejected_f2': n_rejected_f2,
        'n_backed_up': n_backed_up,
        'n_recovered_from_backup': n_recovered_from_backup,
        'n_completed': n_completed,
        'n_lost': max(0, n_lost),
        'p_blank': max(0, n_lost) / n_customers if n_customers > 0 else 0,
        'file1_wait_mean': np.mean(file1_wait_times) if file1_wait_times else 0,
        'file1_system_mean': np.mean(file1_system_times) if file1_system_times else 0,
        'file2_wait_mean': np.mean(file2_wait_times) if file2_wait_times else 0,
        'file2_system_mean': np.mean(file2_system_times) if file2_system_times else 0,
        'backup_time_mean': np.mean(backup_times) if backup_times else 0,
        'total_system_mean': np.mean(total_system_times) if total_system_times else 0,
        'time_trace': np.array(time_trace),
        'f1_size_trace': np.array(f1_size_trace),
        'f2_size_trace': np.array(f2_size_trace),
        'backup_size_trace': np.array(backup_size_trace),
    }


def render_backup_scenario(mu_rate1: float, mu_rate2: float, n_servers: int, K1: int, K2: int):
    """Sc√©nario 2: Impact du backup sur les pages blanches."""
    st.header("M√©canismes de Backup")
    
    st.markdown("""
    <div class="scenario-box">
    <strong>Probl√©matique:</strong> Quand la file 2 est satur√©e, les r√©sultats sont perdus ‚Üí <em>pages blanches</em><br><br>
    <strong>Solution:</strong> Sauvegarder les r√©sultats de moulinettage <em>avant</em> l'envoi vers la file 2.<br>
    Si la file 2 rejette, les donn√©es peuvent √™tre r√©cup√©r√©es depuis le backup.
    </div>
    """, unsafe_allow_html=True)
    
    # Sch√©ma du syst√®me avec backup
    st.subheader("Architecture avec Backup")
    
    fig_arch = go.Figure()
    y_base = 0.5
    
    # File 1
    fig_arch.add_trace(go.Scatter(x=[0], y=[y_base], mode='markers',
        marker=dict(size=40, color='#2ecc71'), showlegend=False))
    fig_arch.add_annotation(x=0, y=y_base-0.2, text="√âtudiants", showarrow=False)
    
    fig_arch.add_annotation(x=0.4, y=y_base, ax=0.1, ay=y_base, xref="x", yref="y", axref="x", ayref="y",
                           showarrow=True, arrowhead=2)
    
    fig_arch.add_trace(go.Scatter(x=[0.8], y=[y_base], mode='markers',
        marker=dict(size=50, color='#3498db', symbol='square'), showlegend=False))
    fig_arch.add_annotation(x=0.8, y=y_base-0.2, text=f"File 1\n(M/M/{n_servers}/K‚ÇÅ)", showarrow=False)
    
    # Backup (point de sauvegarde)
    fig_arch.add_annotation(x=1.3, y=y_base, ax=1.0, ay=y_base, xref="x", yref="y", axref="x", ayref="y",
                           showarrow=True, arrowhead=2)
    
    fig_arch.add_trace(go.Scatter(x=[1.6], y=[y_base], mode='markers',
        marker=dict(size=50, color='#f39c12', symbol='diamond'), showlegend=False))
    fig_arch.add_annotation(x=1.6, y=y_base-0.2, text="BACKUP\n(sauvegarde)", showarrow=False, font=dict(color='#f39c12'))
    
    # Fl√®che vers File 2
    fig_arch.add_annotation(x=2.1, y=y_base, ax=1.8, ay=y_base, xref="x", yref="y", axref="x", ayref="y",
                           showarrow=True, arrowhead=2)
    
    # File 2
    fig_arch.add_trace(go.Scatter(x=[2.4], y=[y_base], mode='markers',
        marker=dict(size=50, color='#9b59b6', symbol='square'), showlegend=False))
    fig_arch.add_annotation(x=2.4, y=y_base-0.2, text=f"File 2\n(M/M/1/K‚ÇÇ)", showarrow=False)
    
    # Fl√®che de r√©cup√©ration (backup -> retry)
    fig_arch.add_trace(go.Scatter(x=[1.6, 2.0, 2.4], y=[y_base+0.25, y_base+0.35, y_base+0.25], 
        mode='lines', line=dict(color='#e74c3c', dash='dash', width=2), showlegend=False))
    fig_arch.add_annotation(x=2.0, y=y_base+0.4, text="R√©cup√©ration si rejet", showarrow=False, font=dict(color='#e74c3c', size=10))
    
    # Frontend
    fig_arch.add_annotation(x=2.9, y=y_base, ax=2.6, ay=y_base, xref="x", yref="y", axref="x", ayref="y",
                           showarrow=True, arrowhead=2)
    
    fig_arch.add_trace(go.Scatter(x=[3.2], y=[y_base], mode='markers',
        marker=dict(size=40, color='#27ae60'), showlegend=False))
    fig_arch.add_annotation(x=3.2, y=y_base-0.2, text="R√©sultat\naffich√©", showarrow=False)
    
    fig_arch.update_layout(
        height=250,
        xaxis=dict(showgrid=False, showticklabels=False, range=[-0.3, 3.5]),
        yaxis=dict(showgrid=False, showticklabels=False, range=[0, 1]),
        margin=dict(l=20, r=20, t=30, b=20),
    )
    st.plotly_chart(apply_dark_theme(fig_arch), use_container_width=True)
    
    st.subheader("Configuration de la simulation")
    
    col_config1, col_config2, col_config3 = st.columns(3)
    
    with col_config1:
        lambda_rate = st.slider("Œª - Taux d'arriv√©e (tags/min)", 10.0, 100.0, 40.0, key="backup_lambda")
    
    with col_config2:
        n_customers = st.number_input("Nombre de clients", 100, 10000, 2000, step=100, key="backup_sim_n")
    
    with col_config3:
        n_runs = st.number_input("Nombre de r√©p√©titions", 1, 20, 5, key="backup_sim_runs")
    
    mu_backup = 50.0  # Valeur fixe pour le taux de sauvegarde
    
    st.divider()
    
    # Section Simulation Monte Carlo avec Backup
    st.subheader("Simulation Monte Carlo - Analyse de sensibilit√© du backup")
    
    st.markdown("""
    La simulation compare automatiquement 5 valeurs de probabilit√© de backup:
    - **P=0%**: Aucun backup (r√©f√©rence)
    - **P=25%**: Backup al√©atoire faible
    - **P=50%**: Backup al√©atoire moyen
    - **P=75%**: Backup al√©atoire √©lev√©
    - **P=100%**: Backup syst√©matique (optimal)
    """)
    
    run_backup_sim = st.button("Lancer simulation compl√®te", type="primary", key="backup_sim_run", use_container_width=True)
    
    if run_backup_sim:
        with st.spinner("Simulation en cours pour P = 0%, 25%, 50%, 75%, 100%..."):
            # Valeurs de P √† tester
            p_values = [0.0, 0.25, 0.5, 0.75, 1.0]
            all_results = {p: [] for p in p_values}
            
            # Ex√©cuter les simulations pour chaque valeur de P
            for p_val in p_values:
                for run in range(n_runs):
                    res = simulate_waterfall_with_backup(
                        lambda_rate, mu_rate1, mu_rate2, mu_backup,
                        n_servers, K1, K2, n_customers, p_backup=p_val, seed=run
                    )
                    all_results[p_val].append(res)
                
            # Affichage des r√©sultats - Tableau comparatif
            st.markdown("### R√©sultats comparatifs")
            
            # Cr√©er le tableau de r√©sultats
            summary_data = []
            for p_val in p_values:
                results = all_results[p_val]
                summary_data.append({
                    'Probabilit√© P': f"{p_val:.0%}",
                    'Pages blanches (%)': f"{np.mean([r['p_blank'] for r in results]) * 100:.3f}",
                    'Clients perdus': f"{np.mean([r['n_lost'] for r in results]):.1f}",
                    'R√©cup√©r√©s backup': f"{np.mean([r['n_recovered_from_backup'] for r in results]):.1f}",
                    'Sauvegard√©s': f"{np.mean([r['n_backed_up'] for r in results]):.1f}",
                    'Rejets File 1': f"{np.mean([r['n_rejected_f1'] for r in results]):.1f}",
                    'Rejets File 2': f"{np.mean([r['n_rejected_f2'] for r in results]):.1f}",
                    'Temps moyen (min)': f"{np.mean([r['total_system_mean'] for r in results]):.3f}",
                    'Compl√©t√©s': f"{np.mean([r['n_completed'] for r in results]):.1f}"
                })
            
            df_summary = pd.DataFrame(summary_data)
            
            # Afficher le tableau avec style
            st.dataframe(
                df_summary,
                use_container_width=True,
                hide_index=True
            )
            
            st.divider()
            
            # Graphiques de visualisation
            st.markdown("### Visualisations comparatives")
            
            # Graphique 1: Pages blanches et clients perdus
            col_g1, col_g2 = st.columns(2)
            
            with col_g1:
                fig_blank = go.Figure()
                blank_rates = [np.mean([r['p_blank'] for r in all_results[p]]) * 100 for p in p_values]
                
                fig_blank.add_trace(go.Scatter(
                    x=[f"{p:.0%}" for p in p_values],
                    y=blank_rates,
                    mode='lines+markers',
                    name='Pages blanches',
                    line=dict(color='#e74c3c', width=3),
                    marker=dict(size=12),
                    fill='tozeroy'
                ))
                
                fig_blank.update_layout(
                    title="Impact de P sur les pages blanches",
                    xaxis_title="Probabilit√© de backup (P)",
                    yaxis_title="Pages blanches (%)",
                    height=350
                )
                st.plotly_chart(apply_dark_theme(fig_blank), use_container_width=True)
            
            with col_g2:
                fig_recovery = go.Figure()
                
                saved = [np.mean([r['n_backed_up'] for r in all_results[p]]) for p in p_values]
                recovered = [np.mean([r['n_recovered_from_backup'] for r in all_results[p]]) for p in p_values]
                lost = [np.mean([r['n_lost'] for r in all_results[p]]) for p in p_values]
                
                fig_recovery.add_trace(go.Scatter(
                    x=[f"{p:.0%}" for p in p_values],
                    y=saved,
                    mode='lines+markers',
                    name='Sauvegard√©s',
                    line=dict(color='#f39c12', width=2),
                    marker=dict(size=10)
                ))
                
                fig_recovery.add_trace(go.Scatter(
                    x=[f"{p:.0%}" for p in p_values],
                    y=recovered,
                    mode='lines+markers',
                    name='R√©cup√©r√©s du backup',
                    line=dict(color='#27ae60', width=2),
                    marker=dict(size=10)
                ))
                
                fig_recovery.add_trace(go.Scatter(
                    x=[f"{p:.0%}" for p in p_values],
                    y=lost,
                    mode='lines+markers',
                    name='Perdus d√©finitivement',
                    line=dict(color='#c0392b', width=2),
                    marker=dict(size=10)
                ))
                
                fig_recovery.update_layout(
                    title="Efficacit√© du m√©canisme de backup",
                    xaxis_title="Probabilit√© de backup (P)",
                    yaxis_title="Nombre de clients",
                    height=350
                )
                st.plotly_chart(apply_dark_theme(fig_recovery), use_container_width=True)
            
            st.divider()
            
            # Graphique 3: Box plots comparatifs (pleine largeur)
            st.markdown("### Distribution des r√©sultats")
            
            fig_boxes = make_subplots(
                rows=1, cols=2,
                subplot_titles=['Distribution des pages blanches (%)', 'Distribution des temps de s√©jour (min)']
            )
            
            colors = ['#c0392b', '#e67e22', '#f39c12', '#27ae60', '#2ecc71']
            
            for i, p_val in enumerate(p_values):
                results = all_results[p_val]
                blanks = [r['p_blank'] * 100 for r in results]
                times = [r['total_system_mean'] for r in results]
                
                fig_boxes.add_trace(go.Box(
                    y=blanks,
                    name=f"P={p_val:.0%}",
                    marker_color=colors[i],
                    showlegend=True
                ), row=1, col=1)
                
                fig_boxes.add_trace(go.Box(
                    y=times,
                    name=f"P={p_val:.0%}",
                    marker_color=colors[i],
                    showlegend=False
                ), row=1, col=2)
            
            fig_boxes.update_yaxes(title_text="Pages blanches (%)", row=1, col=1)
            fig_boxes.update_yaxes(title_text="Temps (min)", row=1, col=2)
            fig_boxes.update_layout(height=450, showlegend=True)
            
            st.plotly_chart(apply_dark_theme(fig_boxes), use_container_width=True)
            
            st.divider()
            
            # Graphique 4: √âvolution temporelle (derni√®re simulation avec P=100%)
            last_res = all_results[1.0][-1]  # Derni√®re simulation avec P=100%
            if len(last_res['time_trace']) > 0:
                st.markdown("### √âvolution temporelle (simulation avec P=100%)")
                
                # Sous-√©chantillonnage si trop de points
                max_points = 1000
                step = max(1, len(last_res['time_trace']) // max_points)
                
                fig_trace = make_subplots(
                    rows=2, cols=1, 
                    shared_xaxes=True,
                    subplot_titles=['Taille des files d\'attente', 'Donn√©es en backup'],
                    vertical_spacing=0.12
                )
                
                fig_trace.add_trace(go.Scatter(
                    x=last_res['time_trace'][::step], 
                    y=last_res['f1_size_trace'][::step],
                    name='File 1 (Moulinette)', 
                    line=dict(color='#3498db', width=2)
                ), row=1, col=1)
                
                fig_trace.add_trace(go.Scatter(
                    x=last_res['time_trace'][::step], 
                    y=last_res['f2_size_trace'][::step],
                    name='File 2 (Envoi r√©sultats)', 
                    line=dict(color='#9b59b6', width=2)
                ), row=1, col=1)
                
                fig_trace.add_trace(go.Scatter(
                    x=last_res['time_trace'][::step], 
                    y=last_res['backup_size_trace'][::step],
                    name='Stockage backup', 
                    line=dict(color='#f39c12', width=2),
                    fill='tozeroy',
                    fillcolor='rgba(243, 156, 18, 0.3)'
                ), row=2, col=1)
                
                fig_trace.update_xaxes(title_text="Temps (min)", row=2, col=1)
                fig_trace.update_yaxes(title_text="Nb clients en file", row=1, col=1)
                fig_trace.update_yaxes(title_text="Nb en backup", row=2, col=1)
                fig_trace.update_layout(height=500, showlegend=True)
                
                st.plotly_chart(apply_dark_theme(fig_trace), use_container_width=True)
    
    st.divider()
    
    st.info("""
    **M√©canisme de backup impl√©ment√©:**
    1. Les r√©sultats de moulinettage sont sauvegard√©s **avant** l'envoi vers la file 2
    2. Si la file 2 rejette (pleine), les donn√©es sont **r√©cup√©r√©es depuis le backup**
    3. Une nouvelle tentative d'envoi est planifi√©e jusqu'au succ√®s
    4. Cela √©limine (presque) totalement les pages blanches
    """)


# ==============================================================================
# CHANNELS & DAMS: POPULATIONS DIFF√âRENCI√âES
# ==============================================================================

def render_channels_dams_tab(mu_rate1: float, n_servers: int, K1: int):
    """Onglet Channels & Dams - Analyse par persona avec files s√©par√©es et prioris√©es."""
    st.header("Sc√©nario 2: Channels & Dams")
    
    # Cr√©er les personas
    personas = PersonaFactory.create_all_personas()
    persona_names = [p.name for p in personas.values()]
    
    # ============================================================================
    # CONFIGURATION
    # ============================================================================
    st.subheader("‚öôÔ∏è Configuration")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        # S√©lection du persona
        selected_persona_name = st.selectbox(
            "üë§ S√©lectionner un Persona",
            persona_names,
            index=0
        )
        
        # R√©cup√©rer le persona s√©lectionn√©
        selected_persona = None
        for p in personas.values():
            if p.name == selected_persona_name:
                selected_persona = p
                break
        
        if selected_persona is None:
            st.error("Persona non trouv√©")
            return
        
        # Afficher les infos du persona
        st.markdown("---")
        st.markdown(f"**Caract√©ristiques de {selected_persona.name}:**")
        st.write(f"‚Ä¢ Population: **{selected_persona.population_size}** utilisateurs")
        st.write(f"‚Ä¢ Taux de base: **{selected_persona.base_submission_rate:.2f}** tags/h/user")
        st.write(f"‚Ä¢ Complexit√©: **{selected_persona.avg_test_complexity:.2f}** min/job")
        
        service_rate_per_server = 1.0 / selected_persona.avg_test_complexity
        st.write(f"‚Ä¢ Vitesse traitement: **{service_rate_per_server:.1f}** jobs/min/serveur")
    
    with col2:
        # Param√®tres de simulation
        st.markdown("**Param√®tres de simulation:**")
        
        simulation_duration = st.slider("Dur√©e de simulation (heures)", 1, 24, 4)
        start_hour = st.slider("Heure de d√©but", 0, 23, 14)
        num_servers = st.slider("Nombre de serveurs", 1, 50, n_servers)
        
        # Calculer la capacit√© totale
        total_capacity = num_servers * service_rate_per_server
        st.metric("Capacit√© totale", f"{total_capacity:.1f} jobs/min")
    
    st.divider()
    
    # ============================================================================
    # SIMULATION PERSONNALIS√âE PAR PERSONA
    # ============================================================================
    st.subheader(f"üìä Simulation pour {selected_persona.name}")
    
    # G√©n√©rer les donn√©es de simulation
    time_steps = simulation_duration * 60  # minutes
    times = np.arange(time_steps)
    
    # Comportement diff√©rent selon le persona
    if selected_persona.student_type == StudentType.PREPA:
        # PR√âPA: Burst de tous les tags en 15 minutes au d√©but, puis traitement
        st.info("üéì **Mode Pr√©pa**: Tous les √©tudiants rendent en m√™me temps (burst de 15 min au d√©but)")
        
        arrivals = np.zeros(time_steps)
        burst_duration = 15  # 15 minutes de burst
        
        # Tous les pr√©pas rendent pendant les 15 premi√®res minutes
        total_submissions = selected_persona.population_size  # 1 tag par √©tudiant
        arrivals_per_minute = total_submissions / burst_duration
        
        for t in range(min(burst_duration, time_steps)):
            arrivals[t] = arrivals_per_minute
        
    else:
        # ING√âNIEUR et ADMIN: Flux continu bas√© sur le taux d'arriv√©e horaire avec variabilit√©
        arrivals = np.zeros(time_steps)
        np.random.seed(42)  # Pour reproductibilit√©
        
        for t in range(time_steps):
            current_hour = (start_hour + t // 60) % 24
            # Taux d'arriv√©e en jobs/min (conversion depuis jobs/heure)
            hourly_rate = selected_persona.get_arrival_rate(current_hour)
            base_rate = hourly_rate / 60.0
            
            # Ajouter de la variabilit√© al√©atoire
            # Pour les ing√©nieurs: variance plus √©lev√©e (comportement plus erratique)
            # Pour admin: variance plus faible (comportement plus r√©gulier)
            if selected_persona.student_type == StudentType.INGENIEUR:
                # Variabilit√© ¬±40% autour du taux de base
                noise_factor = np.random.uniform(0.6, 1.4)
            else:
                # Variabilit√© ¬±20% pour admin
                noise_factor = np.random.uniform(0.8, 1.2)
            
            arrivals[t] = base_rate * noise_factor
    
    # Simulation de la file
    queue_length = np.zeros(time_steps)
    cumulative_arrivals = np.zeros(time_steps)
    cumulative_served = np.zeros(time_steps)
    queue = 0.0
    total_arrived = 0.0
    total_served = 0.0
    
    for t in range(time_steps):
        # 1. Traiter les jobs (capacit√© de service)
        served = min(queue, total_capacity)
        queue = max(0, queue - served)
        total_served += served
        
        # 2. Ajouter les nouvelles arriv√©es
        queue += arrivals[t]
        total_arrived += arrivals[t]
        
        queue_length[t] = queue
        cumulative_arrivals[t] = total_arrived
        cumulative_served[t] = total_served
    
    # Calcul du temps d'attente estim√©
    wait_times = np.where(total_capacity > 0, queue_length / total_capacity, 0)
    
    # ============================================================================
    # GRAPHIQUES
    # ============================================================================
    
    # Graphique principal: File + Arriv√©es vs Capacit√©
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=(
            f'√âvolution de la file d\'attente - {selected_persona.name}',
            'Taux d\'arriv√©e vs Capacit√© de traitement'
        ),
        vertical_spacing=0.15,
        row_heights=[0.6, 0.4]
    )
    
    # File d'attente
    fig.add_trace(go.Scatter(
        x=times, y=queue_length,
        mode='lines',
        name='Jobs en file',
        line=dict(width=3, color='#667eea'),
        fill='tozeroy',
        fillcolor='rgba(102, 126, 234, 0.3)'
    ), row=1, col=1)
    
    # Arriv√©es
    fig.add_trace(go.Scatter(
        x=times, y=arrivals,
        mode='lines',
        name='Arriv√©es (jobs/min)',
        line=dict(width=2, color='#ff6b6b')
    ), row=2, col=1)
    
    # Capacit√© (ligne horizontale)
    fig.add_trace(go.Scatter(
        x=times, y=[total_capacity] * time_steps,
        mode='lines',
        name=f'Capacit√© ({total_capacity:.1f} jobs/min)',
        line=dict(width=3, color='#00ff88', dash='dash')
    ), row=2, col=1)
    
    fig.update_layout(
        height=600,
        hovermode='x unified',
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
    )
    fig.update_xaxes(title_text='Temps (minutes)', row=2, col=1)
    fig.update_yaxes(title_text='Jobs en attente', row=1, col=1)
    fig.update_yaxes(title_text='Jobs/min', row=2, col=1)
    
    st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
    
    # ============================================================================
    # M√âTRIQUES
    # ============================================================================
    st.markdown("### üìã M√©triques")
    
    avg_arrival = np.mean(arrivals)
    total_arrivals_sum = np.sum(arrivals)
    utilization = (avg_arrival / total_capacity * 100) if total_capacity > 0 else 0
    max_queue = max(queue_length)
    final_queue = queue_length[-1]
    avg_wait = np.mean(wait_times)
    
    # Temps pour vider la file (pour Pr√©pa)
    time_to_empty = None
    for t in range(time_steps):
        if queue_length[t] < 0.5 and t > 15:  # File presque vide apr√®s le burst
            time_to_empty = t
            break
    
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        st.metric("Total arriv√©es", f"{total_arrivals_sum:.0f} jobs")
    with col2:
        st.metric("Capacit√©", f"{total_capacity:.1f} j/min")
    with col3:
        st.metric("File max", f"{max_queue:.0f}")
    with col4:
        if time_to_empty:
            st.metric("Temps vidage", f"{time_to_empty} min")
        else:
            st.metric("Attente moy", f"{avg_wait:.1f} min")
    with col5:
        if final_queue < 1:
            st.metric("File finale", f"{final_queue:.0f}", delta="Stable ‚úÖ")
        else:
            st.metric("File finale", f"{final_queue:.0f}", delta="Accumulation ‚ö†Ô∏è", delta_color="inverse")
    
    # Analyse de stabilit√©
    st.markdown("---")
    if selected_persona.student_type == StudentType.PREPA:
        if time_to_empty and time_to_empty < time_steps:
            st.success(f"‚úÖ **File vid√©e en {time_to_empty} minutes** avec {num_servers} serveurs.")
            # Calculer le nombre minimal de serveurs
            min_servers_60min = math.ceil(total_arrivals_sum / (60 * service_rate_per_server))
            st.info(f"üí° Pour vider en ~60 min, il faut minimum {min_servers_60min} serveurs.")
        else:
            st.warning(f"‚ö†Ô∏è La file n'est pas vid√©e apr√®s {simulation_duration}h. Augmentez les serveurs.")
    else:
        if utilization < 100:
            st.success(f"‚úÖ **Syst√®me stable** (œÅ = {utilization:.0f}% < 100%)")
        else:
            servers_needed = math.ceil(avg_arrival / service_rate_per_server) + 1
            st.error(f"‚ùå **Syst√®me instable** (œÅ = {utilization:.0f}% ‚â• 100%). Minimum {servers_needed} serveurs requis.")
    
    # ============================================================================
    # ANALYSE COMPARATIVE: FILES S√âPAR√âES
    # ============================================================================
    st.divider()
    st.subheader("üîÄ Analyse: Files S√©par√©es (Channels)")
    st.markdown("*Chaque population a sa propre file avec serveurs d√©di√©s*")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        servers_prepa = st.number_input("Serveurs Pr√©pa", 1, 50, max(1, num_servers // 2), key="sep_prepa")
    with col2:
        servers_ing = st.number_input("Serveurs Ing√©nieur", 1, 50, max(1, num_servers // 3), key="sep_ing")
    with col3:
        servers_admin = st.number_input("Serveurs Admin", 1, 50, max(1, num_servers // 6), key="sep_admin")
    
    # Simuler les files s√©par√©es
    separate_results = {}
    for persona in personas.values():
        if persona.student_type == StudentType.PREPA:
            servers = servers_prepa
            # Burst pour pr√©pa
            arr = np.zeros(time_steps)
            burst_dur = 15
            arr_per_min = persona.population_size / burst_dur
            for t in range(min(burst_dur, time_steps)):
                arr[t] = arr_per_min
        elif persona.student_type == StudentType.INGENIEUR:
            servers = servers_ing
            arr = np.zeros(time_steps)
            np.random.seed(42)
            for t in range(time_steps):
                current_hour = (start_hour + t // 60) % 24
                base_rate = persona.get_arrival_rate(current_hour) / 60.0
                noise_factor = np.random.uniform(0.6, 1.4)  # Variabilit√© ¬±40%
                arr[t] = base_rate * noise_factor
        else:
            servers = servers_admin
            arr = np.zeros(time_steps)
            np.random.seed(42)
            for t in range(time_steps):
                current_hour = (start_hour + t // 60) % 24
                base_rate = persona.get_arrival_rate(current_hour) / 60.0
                noise_factor = np.random.uniform(0.8, 1.2)  # Variabilit√© ¬±20%
                arr[t] = base_rate * noise_factor
        
        cap = servers * (1.0 / persona.avg_test_complexity)
        q_len = np.zeros(time_steps)
        queue = 0.0
        
        for t in range(time_steps):
            served = min(queue, cap)
            queue = max(0, queue - served)
            queue += arr[t]
            q_len[t] = queue
        
        separate_results[persona.name] = {
            'queue': q_len,
            'arrivals': arr,
            'capacity': cap,
            'servers': servers
        }
    
    # Graphique files s√©par√©es
    fig = go.Figure()
    colors = ['#667eea', '#f093fb', '#00ff88']
    for i, (name, data) in enumerate(separate_results.items()):
        fig.add_trace(go.Scatter(
            x=times, y=data['queue'],
            mode='lines',
            name=f'{name} ({data["servers"]} serv)',
            line=dict(width=2.5, color=colors[i % len(colors)])
        ))
    
    fig.update_layout(
        title='√âvolution des files s√©par√©es',
        xaxis_title='Temps (minutes)',
        yaxis_title='Jobs en attente',
        height=400,
        hovermode='x unified'
    )
    st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
    
    # Tableau m√©triques files s√©par√©es
    sep_data = []
    for name, data in separate_results.items():
        sep_data.append({
            'Population': name,
            'Serveurs': data['servers'],
            'Capacit√©': f"{data['capacity']:.1f} j/min",
            'File max': f"{max(data['queue']):.0f}",
            'File finale': f"{data['queue'][-1]:.0f}"
        })
    st.dataframe(pd.DataFrame(sep_data), use_container_width=True, hide_index=True)
    
    # ============================================================================
    # ANALYSE COMPARATIVE: FILE AVEC PRIORIT√âS
    # ============================================================================
    st.divider()
    st.subheader("‚≠ê Analyse: File avec Priorit√©s")
    st.markdown("*File unique avec traitement prioritaire*")
    
    priority_order = ["Ing√©nieur", "Admin/Assistants", "Pr√©pa (SUP/SPE)"]
    st.info(f"**Ordre de priorit√©:** 1. {priority_order[0]} (priorit√© max) ‚Üí 2. {priority_order[1]} ‚Üí 3. {priority_order[2]} (priorit√© min)")
    
    total_servers_prio = st.slider("Nombre total de serveurs (priorit√©s)", 1, 50, num_servers, key="prio_servers")
    
    # G√©n√©rer les arriv√©es pour chaque persona
    prio_arrivals = {}
    np.random.seed(42)
    for persona in personas.values():
        if persona.student_type == StudentType.PREPA:
            arr = np.zeros(time_steps)
            burst_dur = 15
            arr_per_min = persona.population_size / burst_dur
            for t in range(min(burst_dur, time_steps)):
                arr[t] = arr_per_min
        else:
            arr = np.zeros(time_steps)
            for t in range(time_steps):
                current_hour = (start_hour + t // 60) % 24
                base_rate = persona.get_arrival_rate(current_hour) / 60.0
                # Variabilit√© selon le type
                if persona.student_type == StudentType.INGENIEUR:
                    noise_factor = np.random.uniform(0.6, 1.4)
                else:
                    noise_factor = np.random.uniform(0.8, 1.2)
                arr[t] = base_rate * noise_factor
        prio_arrivals[persona.name] = arr
    
    # Capacit√© moyenne pond√©r√©e
    total_pop = sum(p.population_size for p in personas.values())
    weighted_service = sum((p.population_size / total_pop) * (1.0 / p.avg_test_complexity) for p in personas.values())
    total_capacity_prio = total_servers_prio * weighted_service
    
    # Simulation avec priorit√©s
    prio_queues = {name: 0.0 for name in prio_arrivals}
    prio_queue_lengths = {name: np.zeros(time_steps) for name in prio_arrivals}
    
    for t in range(time_steps):
        # Traiter avec priorit√©s
        remaining_cap = total_capacity_prio
        for prio_name in priority_order:
            if remaining_cap <= 0:
                break
            if prio_name in prio_queues:
                served = min(prio_queues[prio_name], remaining_cap)
                prio_queues[prio_name] = max(0, prio_queues[prio_name] - served)
                remaining_cap -= served
        
        # Ajouter les arriv√©es
        for name in prio_arrivals:
            prio_queues[name] += prio_arrivals[name][t]
            prio_queue_lengths[name][t] = prio_queues[name]
    
    # Graphique files avec priorit√©s
    fig = go.Figure()
    for i, name in enumerate(priority_order):
        if name in prio_queue_lengths:
            fig.add_trace(go.Scatter(
                x=times, y=prio_queue_lengths[name],
                mode='lines',
                name=f'#{i+1} {name}',
                line=dict(width=2.5, color=colors[i % len(colors)])
            ))
    
    fig.update_layout(
        title=f'√âvolution des files avec priorit√©s ({total_servers_prio} serveurs)',
        xaxis_title='Temps (minutes)',
        yaxis_title='Jobs en attente',
        height=400,
        hovermode='x unified'
    )
    st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
    
    # Tableau m√©triques priorit√©s
    prio_data = []
    for i, name in enumerate(priority_order):
        if name in prio_queue_lengths:
            prio_data.append({
                'Priorit√©': f"#{i+1}",
                'Population': name,
                'File max': f"{max(prio_queue_lengths[name]):.0f}",
                'File finale': f"{prio_queue_lengths[name][-1]:.0f}"
            })
    st.dataframe(pd.DataFrame(prio_data), use_container_width=True, hide_index=True)


def generate_arrivals(personas, times, start_hour, enable_burst, burst_time, burst_duration, burst_percentage):
    """G√©n√®re les arriv√©es pour chaque persona."""
    arrivals = {}
    
    for student_type, persona in personas.items():
        arrivals[persona.name] = np.zeros(len(times))
        
        for t in times:
            current_hour = (start_hour + t // 60) % 24
            base_rate = persona.get_arrival_rate(current_hour) / 60.0  # jobs/min
            
            # Ajouter le burst pour les Pr√©pa
            if enable_burst and persona.student_type == StudentType.PREPA:
                if burst_time <= t < burst_time + burst_duration:
                    burst_arrivals = (persona.population_size * burst_percentage / 100) / burst_duration
                    arrivals[persona.name][t] = burst_arrivals
                else:
                    arrivals[persona.name][t] = base_rate
            else:
                arrivals[persona.name][t] = base_rate
    
    return arrivals


def simulate_single_queue(personas, arrivals, times, total_servers):
    """Simule une file unique FIFO avec traitement correct."""
    
    # Taux de service: utiliser un taux moyen pond√©r√© par le volume d'arriv√©es attendu
    total_arrival_rate = sum(np.mean(arrivals[p.name]) for p in personas.values())
    if total_arrival_rate > 0:
        # Pond√©rer par la contribution de chaque persona aux arriv√©es
        weighted_service = 0
        for persona in personas.values():
            arrival_contrib = np.mean(arrivals[persona.name]) / total_arrival_rate
            service_rate_persona = 1.0 / persona.avg_test_complexity  # jobs/min/serveur
            weighted_service += arrival_contrib * service_rate_persona
    else:
        weighted_service = 1.0
    
    total_service_rate = total_servers * weighted_service  # jobs/min totale
    
    # File unique
    queue_length = np.zeros(len(times))
    wait_times = np.zeros(len(times))
    in_service = np.zeros(len(times))
    total_arrivals = np.zeros(len(times))
    
    for name in arrivals:
        total_arrivals += arrivals[name]
    
    queue = 0.0  # Jobs en attente (pas en service)
    
    for t in times:
        # 1. D'abord traiter les jobs (servir)
        served = min(queue, total_service_rate)
        queue = max(0, queue - served)
        
        # 2. Ensuite ajouter les nouvelles arriv√©es
        queue += total_arrivals[t]
        
        queue_length[t] = queue
        # Temps d'attente estim√© = jobs en file / capacit√© de service
        wait_times[t] = queue / total_service_rate if total_service_rate > 0 else 0
        in_service[t] = min(served, total_servers)
    
    return {
        'queue_lengths': {'Total': queue_length},
        'wait_times': {'Total': wait_times},
        'arrivals': {'Total': total_arrivals},
        'in_service': {'Total': in_service},
        'service_rate': total_service_rate,
        'service_rates': {'Total': total_service_rate}
    }


def simulate_separate_queues(personas, arrivals, times, config):
    """Simule des files s√©par√©es avec traitement correct."""
    
    queue_lengths = {}
    wait_times = {}
    service_rates = {}
    in_service = {}
    
    for student_type, persona in personas.items():
        # D√©terminer le nombre de serveurs
        if persona.student_type == StudentType.PREPA:
            servers = config['servers_prepa']
        elif persona.student_type == StudentType.INGENIEUR:
            servers = config['servers_ing']
        else:
            servers = config['servers_admin']
        
        service_rate_per_server = 1.0 / persona.avg_test_complexity  # jobs/min/serveur
        service_rate = service_rate_per_server * servers  # capacit√© totale jobs/min
        service_rates[persona.name] = service_rate
        
        queue = 0.0
        q_lengths = np.zeros(len(times))
        w_times = np.zeros(len(times))
        in_serv = np.zeros(len(times))
        
        for t in times:
            # 1. D'abord traiter (servir les jobs)
            served = min(queue, service_rate)
            queue = max(0, queue - served)
            
            # 2. Ensuite ajouter les nouvelles arriv√©es
            queue += arrivals[persona.name][t]
            
            q_lengths[t] = queue
            w_times[t] = queue / service_rate if service_rate > 0 else 0
            in_serv[t] = served
        
        queue_lengths[persona.name] = q_lengths
        wait_times[persona.name] = w_times
        in_service[persona.name] = in_serv
    
    return {
        'queue_lengths': queue_lengths,
        'wait_times': wait_times,
        'arrivals': arrivals,
        'in_service': in_service,
        'separate': True,
        'service_rates': service_rates
    }


def simulate_priority_queue(personas, arrivals, times, config):
    """Simule une file avec priorit√©s - traitement correct."""
    
    priority_order = config['priority_order']
    total_servers = config['servers']
    
    # Cr√©er un mapping persona -> priorit√© (0 = plus haute)
    priority_map = {name: i for i, name in enumerate(priority_order)}
    
    # Taux de service par persona (jobs/min/serveur)
    service_rates = {
        persona.name: 1.0 / persona.avg_test_complexity
        for persona in personas.values()
    }
    
    # Capacit√© totale (on utilise un taux moyen pour simplifier)
    avg_service_rate = np.mean(list(service_rates.values()))
    total_capacity = total_servers * avg_service_rate  # jobs/min
    
    # Files par persona (valeurs flottantes pour coh√©rence)
    queues = {name: 0.0 for name in arrivals}
    queue_lengths = {name: np.zeros(len(times)) for name in arrivals}
    wait_times = {name: np.zeros(len(times)) for name in arrivals}
    in_service = {name: np.zeros(len(times)) for name in arrivals}
    
    for t in times:
        # 1. D'abord traiter avec priorit√©s
        remaining_capacity = total_capacity
        
        # Traiter par ordre de priorit√©
        for priority_name in priority_order:
            if remaining_capacity <= 0:
                break
            if priority_name in queues:
                served = min(queues[priority_name], remaining_capacity)
                queues[priority_name] = max(0, queues[priority_name] - served)
                remaining_capacity -= served
                in_service[priority_name][t] = served
        
        # 2. Ensuite ajouter les nouvelles arriv√©es
        for name in arrivals:
            queues[name] += arrivals[name][t]
        
        # Enregistrer les m√©triques
        for name in arrivals:
            queue_lengths[name][t] = queues[name]
            # Temps d'attente estim√©
            wait_times[name][t] = queues[name] / (total_capacity / len(arrivals)) if total_capacity > 0 else 0
    
    return {
        'queue_lengths': queue_lengths,
        'wait_times': wait_times,
        'arrivals': arrivals,
        'in_service': in_service,
        'priority_order': priority_order,
        'service_rates': {name: total_capacity / len(arrivals) for name in arrivals}
    }
    
    return {
        'queue_lengths': queue_lengths,
        'wait_times': wait_times,
        'arrivals': arrivals,
        'priority_order': priority_order
    }


def display_simulation_results(results, times, personas, strategy_config):
    """Affiche les r√©sultats de simulation avec graphiques am√©lior√©s."""
    
    # Graphique principal: √âvolution des files avec zone d'arriv√©e vs capacit√©
    st.markdown("### üìä √âvolution des files d'attente")
    
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=('Jobs en attente dans le temps', 'Arriv√©es vs Capacit√© de traitement'),
        vertical_spacing=0.12,
        row_heights=[0.6, 0.4]
    )
    
    # Couleurs hex pour les graphiques
    colors_hex = ['#667eea', '#f093fb', '#00ff88', '#ff6b6b', '#feca57', '#48dbfb']
    colors_rgba = [
        'rgba(102, 126, 234, 0.2)',
        'rgba(240, 147, 251, 0.2)',
        'rgba(0, 255, 136, 0.2)',
        'rgba(255, 107, 107, 0.2)',
        'rgba(254, 202, 87, 0.2)',
        'rgba(72, 219, 251, 0.2)'
    ]
    
    # Graphique du haut: Longueur des files
    for i, (name, lengths) in enumerate(results['queue_lengths'].items()):
        fig.add_trace(go.Scatter(
            x=times, y=lengths,
            mode='lines',
            name=f"üì¶ {name}",
            line=dict(width=2.5, color=colors_hex[i % len(colors_hex)]),
            fill='tozeroy',
            fillcolor=colors_rgba[i % len(colors_rgba)]
        ), row=1, col=1)
    
    # Graphique du bas: Arriv√©es vs Capacit√©
    total_arrivals = np.zeros(len(times))
    for name, arr in results['arrivals'].items():
        total_arrivals += arr
        fig.add_trace(go.Scatter(
            x=times, y=arr,
            mode='lines',
            name=f"‚Üí Arriv√©es {name}",
            line=dict(width=1.5, dash='dot'),
            showlegend=True
        ), row=2, col=1)
    
    # Ajouter la ligne de capacit√©
    if 'service_rates' in results:
        total_capacity = sum(results['service_rates'].values())
        fig.add_trace(go.Scatter(
            x=times, y=[total_capacity] * len(times),
            mode='lines',
            name=f"‚ö° Capacit√© totale ({total_capacity:.1f} jobs/min)",
            line=dict(width=3, color='#00ff88', dash='solid')
        ), row=2, col=1)
    elif 'service_rate' in results:
        fig.add_trace(go.Scatter(
            x=times, y=[results['service_rate']] * len(times),
            mode='lines',
            name=f"‚ö° Capacit√© ({results['service_rate']:.1f} jobs/min)",
            line=dict(width=3, color='#00ff88', dash='solid')
        ), row=2, col=1)
    
    # Total des arriv√©es
    fig.add_trace(go.Scatter(
        x=times, y=total_arrivals,
        mode='lines',
        name='üìà Total arriv√©es',
        line=dict(width=2.5, color='#ff6b6b')
    ), row=2, col=1)
    
    fig.update_layout(
        height=700,
        hovermode='x unified',
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
    )
    fig.update_xaxes(title_text='Temps (minutes)', row=2, col=1)
    fig.update_yaxes(title_text='Jobs en attente', row=1, col=1)
    fig.update_yaxes(title_text='Jobs/min', row=2, col=1)
    
    st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
    
    # Graphique s√©par√©: Temps d'attente
    st.markdown("### ‚è±Ô∏è Temps d'attente estim√©")
    fig = go.Figure()
    for i, (name, waits) in enumerate(results['wait_times'].items()):
        fig.add_trace(go.Scatter(
            x=times, y=waits,
            mode='lines',
            name=name,
            line=dict(width=2.5, color=colors_hex[i % len(colors_hex)])
        ))
    fig.update_layout(
        xaxis_title='Temps (minutes)',
        yaxis_title='Temps d\'attente estim√© (min)',
        height=350,
        hovermode='x unified',
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
    )
    st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
    
    # M√©triques r√©capitulatives
    st.markdown("### üìã M√©triques R√©capitulatives")
    
    metrics_data = []
    for name in results['queue_lengths']:
        max_queue = max(results['queue_lengths'][name])
        avg_queue = np.mean(results['queue_lengths'][name])
        final_queue = results['queue_lengths'][name][-1]
        max_wait = max(results['wait_times'][name])
        avg_wait = np.mean(results['wait_times'][name])
        
        # Calculer taux d'arriv√©e moyen
        avg_arrival = np.mean(results['arrivals'][name])
        
        # Capacit√© de traitement (si disponible)
        capacity_info = ""
        utilization = 0
        if 'service_rates' in results and name in results['service_rates']:
            capacity = results['service_rates'][name]
            utilization = (avg_arrival / capacity * 100) if capacity > 0 else 0
            capacity_info = f"{capacity:.1f} jobs/min"
        
        status = "‚úÖ OK" if final_queue < 5 else ("‚ö†Ô∏è File" if final_queue < 50 else "‚ùå Satur√©")
        
        metrics_data.append({
            'Population': name,
            'Arriv√©e moy': f"{avg_arrival:.2f} j/min",
            'Capacit√©': capacity_info if capacity_info else 'N/A',
            'œÅ (charge)': f"{utilization:.0f}%" if capacity_info else 'N/A',
            'File max': f"{max_queue:.0f}",
            'File finale': f"{final_queue:.0f}",
            'Attente moy': f"{avg_wait:.1f} min",
            'Statut': status
        })
    
    df_metrics = pd.DataFrame(metrics_data)
    st.dataframe(df_metrics, use_container_width=True, hide_index=True)
    
    # R√©sum√© global
    st.markdown("### üéØ R√©sum√© Global")
    
    if 'service_rates' in results:
        total_capacity = sum(results['service_rates'].values())
    elif 'service_rate' in results:
        total_capacity = results['service_rate']
    else:
        total_capacity = 0
    
    total_arrival = sum(np.mean(results['arrivals'][name]) for name in results['arrivals'])
    total_max_queue = sum(max(results['queue_lengths'][name]) for name in results['queue_lengths'])
    total_final_queue = sum(results['queue_lengths'][name][-1] for name in results['queue_lengths'])
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        delta_color = "normal" if total_capacity > total_arrival else "inverse"
        st.metric("Capacit√© totale", f"{total_capacity:.1f} jobs/min")
    with col2:
        st.metric("Charge moyenne", f"{total_arrival:.1f} jobs/min")
    with col3:
        utilization = (total_arrival / total_capacity * 100) if total_capacity > 0 else 0
        st.metric("Utilisation œÅ", f"{utilization:.0f}%")
    with col4:
        if total_final_queue < 5:
            st.metric("File finale", f"{total_final_queue:.0f}", delta="Stable ‚úÖ")
        else:
            st.metric("File finale", f"{total_final_queue:.0f}", delta="Accumulation ‚ö†Ô∏è", delta_color="inverse")


def run_all_strategies_comparison(personas, total_servers, duration_hours, start_hour,
                                  enable_burst, burst_time, burst_duration, burst_percentage,
                                  separate_config):
    """Compare toutes les strat√©gies en une seule fois - affichage vertical."""
    
    st.markdown("---")
    st.subheader("üîÑ Comparaison de Toutes les Strat√©gies")
    
    with st.spinner("Simulation en cours pour toutes les strat√©gies..."):
        # G√©n√©rer les arriv√©es une seule fois
        time_steps = duration_hours * 60
        times = np.arange(time_steps)
        arrivals = generate_arrivals(personas, times, start_hour, enable_burst,
                                    burst_time, burst_duration, burst_percentage)
        
        # Configs par d√©faut si pas de separate_config
        if separate_config is None:
            separate_config = {
                'type': 'separate',
                'servers_prepa': max(1, int(total_servers * 0.5)),
                'servers_ing': max(1, int(total_servers * 0.4)),
                'servers_admin': max(1, int(total_servers * 0.1))
            }
        
        # Simuler chaque strat√©gie
        results_single = simulate_single_queue(personas, arrivals, times, total_servers)
        results_separate = simulate_separate_queues(personas, arrivals, times, separate_config)
        results_priority = simulate_priority_queue(personas, arrivals, times, {
            'type': 'priority',
            'servers': total_servers,
            'priority_order': ["Admin/Assistants", "Ing√©nieur", "Pr√©pa (SUP/SPE)"]
        })
        
        # ========================================================================
        # AFFICHAGE VERTICAL - Chaque strat√©gie l'une apr√®s l'autre
        # ========================================================================
        
        # Tableau r√©capitulatif en haut
        st.markdown("### üìä Tableau Comparatif Rapide")
        
        comparison_data = []
        
        # File unique
        total_max_queue_single = max(results_single['queue_lengths']['Total'])
        total_final_queue_single = results_single['queue_lengths']['Total'][-1]
        total_avg_wait_single = np.mean(results_single['wait_times']['Total'])
        comparison_data.append({
            'Strat√©gie': 'üì¶ File Unique',
            'File max': f"{total_max_queue_single:.0f}",
            'File finale': f"{total_final_queue_single:.0f}",
            'Attente moy': f"{total_avg_wait_single:.2f} min",
            '√âquit√©': 'FIFO strict'
        })
        
        # Files s√©par√©es
        sep_max = max(max(v) for v in results_separate['queue_lengths'].values())
        sep_final = sum(v[-1] for v in results_separate['queue_lengths'].values())
        sep_avg = np.mean([np.mean(v) for v in results_separate['wait_times'].values()])
        comparison_data.append({
            'Strat√©gie': 'üîÄ Files S√©par√©es',
            'File max': f"{sep_max:.0f}",
            'File finale': f"{sep_final:.0f}",
            'Attente moy': f"{sep_avg:.2f} min",
            '√âquit√©': 'Isolation par groupe'
        })
        
        # Priorit√©s
        prio_max = max(max(v) for v in results_priority['queue_lengths'].values())
        prio_final = sum(v[-1] for v in results_priority['queue_lengths'].values())
        prio_avg = np.mean([np.mean(v) for v in results_priority['wait_times'].values()])
        comparison_data.append({
            'Strat√©gie': '‚≠ê File avec Priorit√©s',
            'File max': f"{prio_max:.0f}",
            'File finale': f"{prio_final:.0f}",
            'Attente moy': f"{prio_avg:.2f} min",
            '√âquit√©': 'Favorise prioritaires'
        })
        
        df_comparison = pd.DataFrame(comparison_data)
        st.dataframe(df_comparison, use_container_width=True, hide_index=True)
        
        st.divider()
        
        # ========================================================================
        # STRAT√âGIE 1: File Unique
        # ========================================================================
        st.markdown("## üì¶ Strat√©gie 1: File Unique (FIFO)")
        st.markdown("*Tous les serveurs traitent tous les jobs sans distinction*")
        
        # Graphique √©volution de la file
        fig = make_subplots(
            rows=2, cols=1,
            subplot_titles=('√âvolution de la file', 'Arriv√©es vs Capacit√©'),
            vertical_spacing=0.15,
            row_heights=[0.6, 0.4]
        )
        
        fig.add_trace(go.Scatter(
            x=times, y=results_single['queue_lengths']['Total'],
            mode='lines',
            name='Jobs en file',
            line=dict(width=3, color='#667eea'),
            fill='tozeroy',
            fillcolor='rgba(102, 126, 234, 0.3)'
        ), row=1, col=1)
        
        # Arriv√©es et capacit√©
        fig.add_trace(go.Scatter(
            x=times, y=results_single['arrivals']['Total'],
            mode='lines',
            name='Arriv√©es',
            line=dict(width=2, color='#ff6b6b')
        ), row=2, col=1)
        
        capacity = results_single['service_rate']
        fig.add_trace(go.Scatter(
            x=times, y=[capacity] * len(times),
            mode='lines',
            name=f'Capacit√© ({capacity:.1f} j/min)',
            line=dict(width=3, color='#00ff88', dash='dash')
        ), row=2, col=1)
        
        fig.update_layout(height=500, hovermode='x unified')
        fig.update_xaxes(title_text='Temps (min)', row=2, col=1)
        fig.update_yaxes(title_text='Jobs', row=1, col=1)
        fig.update_yaxes(title_text='Jobs/min', row=2, col=1)
        st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
        
        # M√©triques
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("File max", f"{total_max_queue_single:.0f}")
        col2.metric("File finale", f"{total_final_queue_single:.0f}")
        col3.metric("Attente moy", f"{total_avg_wait_single:.2f} min")
        col4.metric("Capacit√©", f"{capacity:.1f} j/min")
        
        st.divider()
        
        # ========================================================================
        # STRAT√âGIE 2: Files S√©par√©es
        # ========================================================================
        st.markdown("## üîÄ Strat√©gie 2: Files S√©par√©es (Channels)")
        st.markdown("*Chaque population a sa propre file avec serveurs d√©di√©s*")
        
        # Graphique avec toutes les files
        fig = make_subplots(
            rows=2, cols=1,
            subplot_titles=('√âvolution des files par population', 'Arriv√©es par population'),
            vertical_spacing=0.15,
            row_heights=[0.6, 0.4]
        )
        
        colors = ['#667eea', '#f093fb', '#00ff88']
        for i, (name, lengths) in enumerate(results_separate['queue_lengths'].items()):
            fig.add_trace(go.Scatter(
                x=times, y=lengths,
                mode='lines',
                name=f'{name}',
                line=dict(width=2.5, color=colors[i % len(colors)])
            ), row=1, col=1)
        
        for i, (name, arr) in enumerate(results_separate['arrivals'].items()):
            fig.add_trace(go.Scatter(
                x=times, y=arr,
                mode='lines',
                name=f'Arriv√©es {name}',
                line=dict(width=1.5, color=colors[i % len(colors)], dash='dot'),
                showlegend=False
            ), row=2, col=1)
        
        fig.update_layout(height=500, hovermode='x unified')
        fig.update_xaxes(title_text='Temps (min)', row=2, col=1)
        fig.update_yaxes(title_text='Jobs en file', row=1, col=1)
        fig.update_yaxes(title_text='Jobs/min', row=2, col=1)
        st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
        
        # Info sur allocation
        st.info(f"**Allocation:** Pr√©pa: {separate_config['servers_prepa']} serveurs | "
                f"Ing√©nieur: {separate_config['servers_ing']} serveurs | "
                f"Admin: {separate_config['servers_admin']} serveurs")
        
        # Tableau d√©taill√© par population
        sep_metrics = []
        for name in results_separate['queue_lengths']:
            max_q = max(results_separate['queue_lengths'][name])
            final_q = results_separate['queue_lengths'][name][-1]
            avg_wait = np.mean(results_separate['wait_times'][name])
            cap = results_separate['service_rates'][name]
            arr = np.mean(results_separate['arrivals'][name])
            rho = (arr / cap * 100) if cap > 0 else 0
            sep_metrics.append({
                'Population': name,
                'Capacit√©': f"{cap:.1f} j/min",
                'Arriv√©e moy': f"{arr:.2f} j/min",
                'œÅ': f"{rho:.0f}%",
                'File max': f"{max_q:.0f}",
                'File finale': f"{final_q:.0f}",
                'Attente moy': f"{avg_wait:.2f} min"
            })
        st.dataframe(pd.DataFrame(sep_metrics), use_container_width=True, hide_index=True)
        
        st.divider()
        
        # ========================================================================
        # STRAT√âGIE 3: File avec Priorit√©s
        # ========================================================================
        st.markdown("## ‚≠ê Strat√©gie 3: File avec Priorit√©s")
        st.markdown("*File unique avec traitement prioritaire (Admin > Ing√©nieur > Pr√©pa)*")
        
        # Graphique
        fig = make_subplots(
            rows=2, cols=1,
            subplot_titles=('√âvolution des files par population', 'Arriv√©es par population'),
            vertical_spacing=0.15,
            row_heights=[0.6, 0.4]
        )
        
        for i, (name, lengths) in enumerate(results_priority['queue_lengths'].items()):
            fig.add_trace(go.Scatter(
                x=times, y=lengths,
                mode='lines',
                name=f'{name}',
                line=dict(width=2.5, color=colors[i % len(colors)])
            ), row=1, col=1)
        
        for i, (name, arr) in enumerate(results_priority['arrivals'].items()):
            fig.add_trace(go.Scatter(
                x=times, y=arr,
                mode='lines',
                name=f'Arriv√©es {name}',
                line=dict(width=1.5, color=colors[i % len(colors)], dash='dot'),
                showlegend=False
            ), row=2, col=1)
        
        fig.update_layout(height=500, hovermode='x unified')
        fig.update_xaxes(title_text='Temps (min)', row=2, col=1)
        fig.update_yaxes(title_text='Jobs en file', row=1, col=1)
        fig.update_yaxes(title_text='Jobs/min', row=2, col=1)
        st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
        
        # Tableau d√©taill√©
        prio_metrics = []
        for name in results_priority['queue_lengths']:
            max_q = max(results_priority['queue_lengths'][name])
            final_q = results_priority['queue_lengths'][name][-1]
            avg_wait = np.mean(results_priority['wait_times'][name])
            priority_rank = results_priority['priority_order'].index(name) + 1 if name in results_priority['priority_order'] else '-'
            prio_metrics.append({
                'Population': name,
                'Priorit√©': f"#{priority_rank}",
                'File max': f"{max_q:.0f}",
                'File finale': f"{final_q:.0f}",
                'Attente moy': f"{avg_wait:.2f} min"
            })
        st.dataframe(pd.DataFrame(prio_metrics), use_container_width=True, hide_index=True)
        
        st.divider()
        
        # ========================================================================
        # GRAPHIQUE COMPARATIF FINAL
        # ========================================================================
        st.markdown("### üìà Comparaison des Temps d'Attente par Population")
        
        fig = go.Figure()
        
        strategies = ['File Unique', 'Files S√©par√©es', 'Priorit√©s']
        
        for persona in personas.values():
            name = persona.name
            
            # File unique - moyenne globale pour tous
            wait_single = np.mean(results_single['wait_times']['Total'])
            
            # Files s√©par√©es
            wait_sep = np.mean(results_separate['wait_times'][name])
            
            # Priorit√©s
            wait_prio = np.mean(results_priority['wait_times'][name])
            
            fig.add_trace(go.Bar(
                name=name,
                x=strategies,
                y=[wait_single, wait_sep, wait_prio]
            ))
        
        fig.update_layout(
            barmode='group',
            yaxis_title='Temps d\'attente moyen (min)',
            height=400,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
        )
        st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
        
        # Recommandation
        st.markdown("### üí° Recommandation")
        
        if prio_avg < sep_avg and prio_avg < total_avg_wait_single:
            st.success("‚úÖ **File avec Priorit√©s** offre les meilleures performances globales tout en favorisant les populations critiques.")
        elif sep_avg < total_avg_wait_single * 0.9:
            st.success("‚úÖ **Files S√©par√©es** offre une meilleure isolation et pr√©visibilit√© pour chaque population.")
        else:
            st.info("‚ÑπÔ∏è **File Unique** est suffisante avec les param√®tres actuels, mais moins r√©siliente aux pics.")


def run_theoretical_analysis(personas, total_servers, strategy_config):
    """Analyse th√©orique sans simulation (M/M/c)."""
    
    st.markdown("---")
    st.subheader("Analyse Th√©orique (Mod√®le M/M/c)")
    
    st.info("Analyse bas√©e sur les taux moyens (14h) sans burst")
    
    if strategy_config['type'] == 'single':
        analyze_single_queue_theoretical(personas, total_servers)
    elif strategy_config['type'] == 'separate':
        analyze_separate_queues_theoretical(personas, strategy_config)
    else:
        st.warning("Analyse th√©orique non disponible pour les files avec priorit√©s (n√©cessite simulation)")


def analyze_single_queue_theoretical(personas, total_servers):
    """Analyse th√©orique file unique."""
    
    total_arrival = sum(p.get_arrival_rate(14) for p in personas.values()) / 60  # jobs/min
    total_pop = sum(p.population_size for p in personas.values())
    weighted_service = sum((p.population_size / total_pop) * (1.0 / p.avg_test_complexity) 
                          for p in personas.values())
    
    rho = total_arrival / (total_servers * weighted_service)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("Taux d'arriv√©e Œª", f"{total_arrival:.2f} jobs/min")
        st.metric("Taux de service Œº", f"{weighted_service:.2f} jobs/min/serveur")
    
    with col2:
        st.metric("Charge œÅ", f"{rho:.2%}")
        st.metric("Serveurs", total_servers)
    
    if rho < 1:
        try:
            queue = GenericQueue(total_arrival, weighted_service, f"M/M/{total_servers}", c=total_servers)
            metrics = queue.compute_theoretical_metrics()
            
            st.success("Syst√®me stable")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("L (jobs dans syst√®me)", f"{metrics.L:.2f}")
            with col2:
                st.metric("Lq (jobs en attente)", f"{metrics.Lq:.2f}")
            with col3:
                st.metric("W (temps total, min)", f"{metrics.W:.2f}")
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Wq (temps attente, min)", f"{metrics.Wq:.2f}")
            with col2:
                st.metric("P0 (proba vide)", f"{metrics.P0:.2%}")
        except Exception as e:
            st.error(f"Erreur de calcul: {e}")
    else:
        st.error("Syst√®me instable (œÅ ‚â• 1)")


def analyze_separate_queues_theoretical(personas, config):
    """Analyse th√©orique files s√©par√©es."""
    
    results_data = []
    
    for student_type, persona in personas.items():
        if persona.student_type == StudentType.PREPA:
            servers = config['servers_prepa']
        elif persona.student_type == StudentType.INGENIEUR:
            servers = config['servers_ing']
        else:
            servers = config['servers_admin']
        
        arrival_rate = persona.get_arrival_rate(14) / 60  # jobs/min
        service_rate = 1.0 / persona.avg_test_complexity
        
        rho = arrival_rate / (servers * service_rate) if servers > 0 else float('inf')
        
        if rho < 1 and servers > 0:
            try:
                queue = GenericQueue(arrival_rate, service_rate, f"M/M/{servers}", c=servers)
                metrics = queue.compute_theoretical_metrics()
                
                results_data.append({
                    'Population': persona.name,
                    'Serveurs': servers,
                    'Œª': f"{arrival_rate:.2f}",
                    'Œº': f"{service_rate:.2f}",
                    'œÅ': f"{rho:.2%}",
                    'Wq (min)': f"{metrics.Wq:.2f}",
                    'Lq': f"{metrics.Lq:.2f}",
                    'Statut': 'OK'
                })
            except:
                results_data.append({
                    'Population': persona.name,
                    'Serveurs': servers,
                    'Œª': f"{arrival_rate:.2f}",
                    'Œº': f"{service_rate:.2f}",
                    'œÅ': f"{rho:.2%}",
                    'Wq (min)': 'Erreur',
                    'Lq': 'Erreur',
                    'Statut': 'Instable'
                })
        else:
            results_data.append({
                'Population': persona.name,
                'Serveurs': servers,
                'Œª': f"{arrival_rate:.2f}",
                'Œº': f"{service_rate:.2f}",
                'œÅ': '‚â•100%',
                'Wq (min)': '‚àû',
                'Lq': '‚àû',
                'Statut': 'Instable'
            })
    
    df = pd.DataFrame(results_data)
    st.dataframe(df, use_container_width=True)


# ==============================================================================
# OPTIMISATION CO√õT / QUALIT√â DE SERVICE
# ==============================================================================

def render_optimization_tab(mu_rate: float, n_servers: int, buffer_size: int):
    """Onglet d'optimisation co√ªt/performance."""
    st.header("Optimisation Co√ªt / Qualit√© de Service")
    
    st.markdown("""
    <div class="formula-box">
    <strong>Fonction objectif:</strong><br>
    min [ Œ± √ó E[T] + Œ≤ √ó Co√ªt(K, c, Œº) ] avec Œ± + Œ≤ = 1
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("""
    - **E[T]** = Temps moyen de s√©jour (temps d'attente + service)
    - **Co√ªt** = Co√ªt serveurs (‚Ç¨/h)
    - **Œ±** = Poids accord√© √† la performance (temps)
    - **Œ≤** = Poids accord√© au co√ªt
    """)
    
    st.divider()
    
    # Configuration
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("Taux d'arriv√©e")
        
        input_mode = st.radio("Source", ["Manuel", "Personas"], horizontal=True)
        
        if input_mode == "Manuel":
            lambda_rate = st.number_input("Œª (soumissions/min)", 1.0, 200.0, 30.0, key="opt_lambda")
        else:
            personas = PersonaFactory.create_all_personas()
            selected = st.multiselect(
                "Populations actives",
                [p.name for p in personas.values()],
                default=[p.name for p in personas.values()]
            )
            
            lambda_rate = 0.0
            for p in personas.values():
                if p.name in selected:
                    lambda_rate += p.get_arrival_rate(14) / 60.0
            
            st.info(f"Œª combin√© = {lambda_rate:.2f} sub/min")
    
    with col2:
        st.subheader("Pond√©ration")
        
        alpha = st.slider("Œ± (poids de performance (temps))", 0.0, 1.0, 0.7, 0.05)
        st.write(f"Œ≤ (poids de co√ªt) = {1-alpha:.2f}")
        
        st.markdown("---")
        st.subheader("Mod√®le de co√ªt")
        
        cost_server = st.number_input("Co√ªt serveur (‚Ç¨/h)", 0.1, 10.0, 0.50, 0.1)
        st.info("Le co√ªt total est calcul√© uniquement √† partir du nombre de serveurs actifs.")
    
    st.divider()
    
    # Param√®tres de recherche
    st.subheader("Espace de recherche")
    
    col1, col2, col3 = st.columns(3)    
    with col1:
        max_servers = st.slider("Max serveurs", 5, 30, 15)
    with col2:
        mu_min = st.number_input("Œº min", 5.0, 20.0, 5.0)
        mu_max = st.number_input("Œº max", 10.0, 50.0, 25.0)
    with col3:
        resolution = st.slider("R√©solution", 10, 40, 20)
    
    if st.button("Lancer l'optimisation", type="primary"):
        run_optimization(lambda_rate, alpha, cost_server,
                        max_servers, mu_min, mu_max, resolution)


def run_optimization(lambda_rate, alpha, cost_server,
                    max_servers, mu_min, mu_max, resolution):
    """Ex√©cute l'optimisation et affiche les heatmaps."""
    
    with st.spinner("Optimisation en cours..."):
        server_range = np.arange(1, max_servers + 1)
        mu_range = np.linspace(mu_min, mu_max, resolution)
        
        Z_cost = np.zeros((len(mu_range), len(server_range)))
        Z_time = np.zeros((len(mu_range), len(server_range)))
        Z_score = np.zeros((len(mu_range), len(server_range)))
        
        for i, mu in enumerate(mu_range):
            for j, c in enumerate(server_range):
                rho = lambda_rate / (c * mu)
                
                if rho >= 1:
                    Z_cost[i, j] = np.nan
                    Z_time[i, j] = np.nan
                    Z_score[i, j] = np.nan
                else:
                    try:
                        queue = GenericQueue(lambda_rate, mu, f"M/M/{int(c)}", c=int(c))
                        metrics = queue.compute_theoretical_metrics()
                        
                        # Co√ªt horaire (uniquement serveurs)
                        total_cost = c * cost_server
                        
                        Z_cost[i, j] = total_cost
                        Z_time[i, j] = metrics.W
                        
                        # Score normalis√© (alpha * co√ªt + (1-alpha) * temps normalis√©)
                        # On normalise le temps pour le mettre √† l'√©chelle avec le co√ªt
                        time_normalized = metrics.W * 10  # Facteur d'√©chelle arbitraire
                        Z_score[i, j] = alpha * total_cost + (1 - alpha) * time_normalized
                        
                    except:
                        Z_cost[i, j] = np.nan
                        Z_time[i, j] = np.nan
                        Z_score[i, j] = np.nan
        
        # Normaliser le score pour affichage
        valid = ~np.isnan(Z_score)
        if valid.any():
            Z_score_norm = (Z_score - np.nanmin(Z_score)) / (np.nanmax(Z_score) - np.nanmin(Z_score))
        else:
            st.error("Aucune configuration stable trouv√©e")
            return
        
        # Afficher les heatmaps
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### Heatmap Co√ªt Total (‚Ç¨/h)")
            fig_cost = go.Figure(data=go.Heatmap(
                z=Z_cost,
                x=server_range,
                y=mu_range,
                colorscale='RdYlGn_r',
                colorbar=dict(title="‚Ç¨/h")
            ))
            fig_cost.update_layout(
                xaxis_title='Nombre de serveurs (c)',
                yaxis_title='Taux de service Œº',
                height=400
            )
            st.plotly_chart(apply_dark_theme(fig_cost), use_container_width=True)
        
        with col2:
            st.markdown("### Heatmap Temps de S√©jour (min)")
            fig_time = go.Figure(data=go.Heatmap(
                z=Z_time,
                x=server_range,
                y=mu_range,
                colorscale='RdYlGn_r',
                colorbar=dict(title="min")
            ))
            fig_time.update_layout(
                xaxis_title='Nombre de serveurs (c)',
                yaxis_title='Taux de service Œº',
                height=400
            )
            st.plotly_chart(apply_dark_theme(fig_time), use_container_width=True)
        
        # Score combin√©
        st.markdown("### Heatmap Score Combin√© (Œ±√óCo√ªt + Œ≤√óTemps)")
        fig_score = go.Figure(data=go.Heatmap(
            z=Z_score_norm,
            x=server_range,
            y=mu_range,
            colorscale='RdYlGn_r',
            colorbar=dict(title="Score")
        ))
        fig_score.update_layout(
            xaxis_title='Nombre de serveurs (c)',
            yaxis_title='Taux de service Œº',
            height=400
        )
        st.plotly_chart(apply_dark_theme(fig_score), use_container_width=True)
        
        # Configuration optimale
        min_idx = np.nanargmin(Z_score)
        min_i, min_j = np.unravel_index(min_idx, Z_score.shape)
        
        opt_mu = mu_range[min_i]
        opt_c = server_range[min_j]
        opt_cost = Z_cost[min_i, min_j]
        opt_time = Z_time[min_i, min_j]
        
        st.success(f"""
        ### Configuration Optimale
        
        | Param√®tre | Valeur |
        |-----------|--------|
        | **Serveurs (c)** | {opt_c} |
        | **Taux service (Œº)** | {opt_mu:.2f}/min |
        | **Co√ªt total** | {opt_cost:.2f} ‚Ç¨/h |
        | **Temps de s√©jour** | {opt_time:.3f} min |
        | **Utilisation (œÅ)** | {lambda_rate/(opt_c*opt_mu):.1%} |
        """)


# ==============================================================================
# AUTO-SCALING
# ==============================================================================

def render_autoscaling_tab(mu_rate1: float, mu_rate2: float, n_servers: int, K1: int, K2: int):
    """Onglet des strat√©gies d'auto-scaling."""
    st.header("Strat√©gies d'Auto-Scaling")
    
    st.markdown("""
    <div class="scenario-box">
    <strong>Objectif:</strong> Adapter dynamiquement le nombre de serveurs √† la charge
    </div>
    """, unsafe_allow_html=True)
    
    # Types de scaling
    scaling_type = st.radio(
        "Strat√©gie de scaling",
        ["Fixe", "Programm√©", "R√©actif", "Pr√©dictif"],
        horizontal=True
    )
    
    st.divider()
    
    if scaling_type == "Fixe":
        st.markdown("""
        **Scaling Fixe:** Nombre constant de serveurs
        
        - Simple √† g√©rer  
        - Pas d'adaptation √† la charge
        """)
        
        st.metric("Serveurs fixes", n_servers)
    
    elif scaling_type == "Programm√©":
        st.markdown("""
        **Scaling Programm√©:** Nombre de serveurs selon l'heure
        
        - Adapt√© aux patterns connus  
        - Ne g√®re pas les pics impr√©vus
        """)
        
        st.subheader("Configuration horaire")
        
        schedule = {}
        cols = st.columns(6)
        for i, h in enumerate([0, 4, 8, 12, 16, 20]):
            with cols[i]:
                schedule[h] = st.number_input(f"{h}h-{h+4}h", 1, 20, n_servers, key=f"sched_{h}")
        
        # Visualisation
        hours = list(range(24))
        servers = []
        for h in hours:
            for start in sorted(schedule.keys(), reverse=True):
                if h >= start:
                    servers.append(schedule[start])
                    break
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=hours, y=servers, mode='lines+markers', fill='tozeroy', name='Serveurs'))
        fig.update_layout(xaxis_title="Heure", yaxis_title="Serveurs", height=300)
        st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
    
    elif scaling_type == "R√©actif":
        st.markdown("""
        **Scaling R√©actif:** Ajustement bas√© sur la charge actuelle
        
        - R√©agit aux pics  
        - Temps de r√©action (cooldown)
        """)
        
        col1, col2 = st.columns(2)
        with col1:
            scale_up = st.slider("Seuil scale-up (œÅ)", 0.5, 0.95, 0.8)
            scale_up_inc = st.number_input("Serveurs √† ajouter", 1, 5, 2)
        with col2:
            scale_down = st.slider("Seuil scale-down (œÅ)", 0.1, 0.5, 0.3)
            scale_down_inc = st.number_input("Serveurs √† retirer", 1, 3, 1)
        
        cooldown = st.slider("Cooldown (min)", 1, 30, 10)
        
        st.markdown(f"""
        **R√®gles:**
        - Si œÅ > {scale_up:.0%} ‚Üí +{scale_up_inc} serveurs
        - Si œÅ < {scale_down:.0%} ‚Üí -{scale_down_inc} serveur(s)
        - D√©lai entre ajustements: {cooldown} min
        """)
    
    elif scaling_type == "Pr√©dictif":
        st.markdown("""
        **Scaling Pr√©dictif:** Anticipation bas√©e sur les donn√©es historiques
        
        - √âvite les temps de r√©action  
        - N√©cessite des donn√©es historiques
        """)
        
        st.info("Le scaling pr√©dictif utilise les patterns de soumission historiques pour anticiper la charge.")
        
        # Simulation d'une pr√©diction
        st.subheader("Pr√©diction de charge (exemple)")
        
        hours = list(range(24))
        predicted_load = [20 + 30 * np.sin((h - 14) * np.pi / 12) ** 2 for h in hours]
        predicted_servers = [max(2, int(load / mu_rate1) + 1) for load in predicted_load]
        
        fig = make_subplots(specs=[[{"secondary_y": True}]])
        fig.add_trace(go.Scatter(x=hours, y=predicted_load, name="Charge pr√©dite", line=dict(color='blue')), secondary_y=False)
        fig.add_trace(go.Scatter(x=hours, y=predicted_servers, name="Serveurs recommand√©s", line=dict(color='green', dash='dash')), secondary_y=True)
        fig.update_layout(height=350)
        fig.update_yaxes(title_text="Charge (tags/min)", secondary_y=False)
        fig.update_yaxes(title_text="Serveurs", secondary_y=True)
        st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
    
    st.divider()
    
    # Comparaison des strat√©gies
    st.subheader("Comparaison des strat√©gies")
    
    comparison_data = {
        'Strat√©gie': ['Fixe', 'Programm√©', 'R√©actif', 'Pr√©dictif'],
        'R√©activit√©': ['‚ùå Aucune', '‚ö†Ô∏è Limit√©e', '‚úÖ Excellente', '‚úÖ Bonne'],
        'Complexit√©': ['‚úÖ Simple', '‚ö†Ô∏è Moyenne', '‚ö†Ô∏è Moyenne', '‚ùå Complexe'],
        'Co√ªt': ['‚úÖ Stable', '‚úÖ Bon si bien planifi√©', '‚úÖ Optimis√©', '‚úÖ Optimis√©'],
        'Pics impr√©vus': ['‚ùå Non g√©r√©', '‚ùå Non g√©r√©', '‚úÖ G√©r√©', '‚ö†Ô∏è Non g√©r√© si mal entra√Æn√©']
    }
    
    st.dataframe(pd.DataFrame(comparison_data), use_container_width=True)


# ==============================================================================
# BENCHMARK DES MOD√àLES
# ==============================================================================

def render_benchmark_tab(mu_rate: float, n_servers: int, buffer_size: int):
    """Onglet de benchmark comparatif des mod√®les de files."""
    st.header("üî¨ Benchmark des Mod√®les de Files d'Attente")
    
    col1, col2 = st.columns([2, 1])
    
    with col2:
        st.subheader("S√©lection des mod√®les")
        
        st.markdown("**Mono-serveur:**")
        show_mm1 = st.checkbox("M/M/1", value=True, key="b_mm1")
        show_md1 = st.checkbox("M/D/1", value=True, key="b_md1")
        show_mg1 = st.checkbox("M/G/1", value=True, key="b_mg1")
        
        st.markdown("**Multi-serveurs:**")
        show_mmc = st.checkbox("M/M/c", value=True, key="b_mmc")
        show_mdc = st.checkbox("M/D/c", value=True, key="b_mdc")
        show_mgc = st.checkbox("M/G/c", value=True, key="b_mgc")
        
        cv_squared = st.slider("CV¬≤ (pour M/G/*)", 0.0, 2.0, 1.0, 0.1)
    
    with col1:
        lambda_rate = st.slider("Œª - Taux d'arriv√©e", 1.0, 100.0, 30.0, key="b_lambda")
        mu_rate_b = st.slider("Œº - Taux de service", 1.0, 50.0, 10.0, key="b_mu")
        
        models_data = []
        
        # Mono-serveur
        if show_mm1 and lambda_rate < mu_rate_b:
            queue = GenericQueue(lambda_rate, mu_rate_b, "M/M/1")
            m = queue.compute_theoretical_metrics()
            models_data.append({'Mod√®le': 'M/M/1', 'L': m.L, 'Lq': m.Lq, 'W': m.W, 'Wq': m.Wq, 'œÅ': m.rho})
        
        if show_md1 and lambda_rate < mu_rate_b:
            queue = GenericQueue(lambda_rate, mu_rate_b, "M/D/1")
            m = queue.compute_theoretical_metrics()
            models_data.append({'Mod√®le': 'M/D/1', 'L': m.L, 'Lq': m.Lq, 'W': m.W, 'Wq': m.Wq, 'œÅ': m.rho})
        
        if show_mg1 and lambda_rate < mu_rate_b:
            queue = GenericQueue(lambda_rate, mu_rate_b, "M/G/1")
            queue.service_variance = cv_squared * (1/mu_rate_b)**2
            m = queue.compute_theoretical_metrics()
            models_data.append({'Mod√®le': f'M/G/1 (CV¬≤={cv_squared})', 'L': m.L, 'Lq': m.Lq, 'W': m.W, 'Wq': m.Wq, 'œÅ': m.rho})
        
        # Multi-serveurs
        if show_mmc and lambda_rate < n_servers * mu_rate_b:
            queue = GenericQueue(lambda_rate, mu_rate_b, f"M/M/{n_servers}", c=n_servers)
            m = queue.compute_theoretical_metrics()
            models_data.append({'Mod√®le': f'M/M/{n_servers}', 'L': m.L, 'Lq': m.Lq, 'W': m.W, 'Wq': m.Wq, 'œÅ': m.rho})
        
        if show_mdc and lambda_rate < n_servers * mu_rate_b:
            queue = GenericQueue(lambda_rate, mu_rate_b, f"M/D/{n_servers}", c=n_servers)
            m = queue.compute_theoretical_metrics()
            models_data.append({'Mod√®le': f'M/D/{n_servers}', 'L': m.L, 'Lq': m.Lq, 'W': m.W, 'Wq': m.Wq, 'œÅ': m.rho})
        
        if show_mgc and lambda_rate < n_servers * mu_rate_b:
            queue = GenericQueue(lambda_rate, mu_rate_b, f"M/G/{n_servers}", c=n_servers)
            queue.service_variance = cv_squared * (1/mu_rate_b)**2
            m = queue.compute_theoretical_metrics()
            models_data.append({'Mod√®le': f'M/G/{n_servers} (CV¬≤={cv_squared})', 'L': m.L, 'Lq': m.Lq, 'W': m.W, 'Wq': m.Wq, 'œÅ': m.rho})
        
        if models_data:
            df = pd.DataFrame(models_data)
            
            st.subheader("Comparaison th√©orique")
            st.dataframe(df.style.format({
                'L': '{:.2f}',
                'Lq': '{:.2f}',
                'W': '{:.4f}',
                'Wq': '{:.4f}',
                'œÅ': '{:.2%}'
            }), use_container_width=True)
            
            # Graphiques
            fig = make_subplots(rows=1, cols=2, subplot_titles=['Temps d\'attente (Wq)', 'Longueur de queue (Lq)'])
            
            colors = px.colors.qualitative.Set2
            
            fig.add_trace(go.Bar(x=df['Mod√®le'], y=df['Wq'], marker_color=colors[:len(df)]), row=1, col=1)
            fig.add_trace(go.Bar(x=df['Mod√®le'], y=df['Lq'], marker_color=colors[:len(df)]), row=1, col=2)
            
            fig.update_layout(height=400, showlegend=False)
            st.plotly_chart(apply_dark_theme(fig), use_container_width=True)
        else:
            st.warning("Aucun mod√®le stable s√©lectionn√©. V√©rifiez que Œª < c√óŒº")
    
    st.divider()
    
    # Simulation comparative
    st.subheader("Simulation Monte Carlo Comparative")
    
    col_s1, col_s2 = st.columns([1, 3])
    
    with col_s1:
        sim_n = st.number_input("Clients", 100, 10000, 2000, key="b_sim_n")
        sim_runs = st.number_input("Runs", 1, 20, 5, key="b_sim_runs")
        run_benchmark = st.button("Lancer benchmark", type="primary")
    
    with col_s2:
        if run_benchmark:
            run_model_benchmark(lambda_rate, mu_rate_b, n_servers, cv_squared, sim_n, sim_runs,
                               show_mm1, show_md1, show_mg1, show_mmc, show_mdc, show_mgc)


def run_model_benchmark(lambda_rate, mu_rate, n_servers, cv_squared, n_customers, n_runs,
                       show_mm1, show_md1, show_mg1, show_mmc, show_mdc, show_mgc):
    """Ex√©cute un benchmark Monte Carlo des mod√®les."""
    
    with st.spinner("Benchmark en cours..."):
        results = []
        
        models_to_test = []
        if show_mm1 and lambda_rate < mu_rate:
            models_to_test.append(("M/M/1", GenericQueue(lambda_rate, mu_rate, "M/M/1")))
        if show_md1 and lambda_rate < mu_rate:
            models_to_test.append(("M/D/1", GenericQueue(lambda_rate, mu_rate, "M/D/1")))
        if show_mg1 and lambda_rate < mu_rate:
            q = GenericQueue(lambda_rate, mu_rate, "M/G/1")
            q.service_variance = cv_squared * (1/mu_rate)**2
            models_to_test.append(("M/G/1", q))
        if show_mmc and lambda_rate < n_servers * mu_rate:
            models_to_test.append((f"M/M/{n_servers}", GenericQueue(lambda_rate, mu_rate, f"M/M/{n_servers}", c=n_servers)))
        if show_mdc and lambda_rate < n_servers * mu_rate:
            models_to_test.append((f"M/D/{n_servers}", GenericQueue(lambda_rate, mu_rate, f"M/D/{n_servers}", c=n_servers)))
        if show_mgc and lambda_rate < n_servers * mu_rate:
            q = GenericQueue(lambda_rate, mu_rate, f"M/G/{n_servers}", c=n_servers)
            q.service_variance = cv_squared * (1/mu_rate)**2
            models_to_test.append((f"M/G/{n_servers}", q))
        
        progress = st.progress(0)
        total = len(models_to_test) * n_runs
        current = 0
        
        for name, queue in models_to_test:
            for run in range(n_runs):
                try:
                    res = queue.simulate(n_customers=n_customers)
                    results.append({
                        'Mod√®le': name,
                        'Run': run + 1,
                        'Wq (sim)': np.mean(res.waiting_times) if len(res.waiting_times) > 0 else 0,
                        'W (sim)': np.mean(res.system_times) if len(res.system_times) > 0 else 0,
                        'Lq max': np.max(res.queue_length_trace) if len(res.queue_length_trace) > 0 else 0
                    })
                except Exception as e:
                    st.warning(f"{name}: {e}")
                
                current += 1
                progress.progress(current / total)
        
        progress.empty()
        
        if results:
            df = pd.DataFrame(results)
            
            # R√©sum√©
            summary = df.groupby('Mod√®le').agg({
                'Wq (sim)': ['mean', 'std'],
                'W (sim)': ['mean', 'std'],
                'Lq max': ['mean', 'std']
            }).round(4)
            
            st.markdown("### R√©sultats")
            
            # Cr√©er un DataFrame plus lisible
            summary_display = pd.DataFrame()
            for col in ['Wq (sim)', 'W (sim)', 'Lq max']:
                mean = summary[col]['mean']
                std = summary[col]['std']
                summary_display[col] = mean.astype(str) + ' ¬± ' + std.astype(str)
            
            st.dataframe(summary_display, use_container_width=True)
            
            # Box plots
            fig = make_subplots(rows=1, cols=2, subplot_titles=['Temps d\'attente', 'Temps de s√©jour'])
            
            for model in df['Mod√®le'].unique():
                model_data = df[df['Mod√®le'] == model]
                fig.add_trace(go.Box(y=model_data['Wq (sim)'], name=model), row=1, col=1)
                fig.add_trace(go.Box(y=model_data['W (sim)'], name=model, showlegend=False), row=1, col=2)
            
            fig.update_layout(height=400)
            st.plotly_chart(apply_dark_theme(fig), use_container_width=True)


if __name__ == "__main__":
    main()
